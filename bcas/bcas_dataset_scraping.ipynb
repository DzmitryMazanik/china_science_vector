{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac07c84",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Building Dataset\"\n",
    "jupyter: python3\n",
    "format:\n",
    "  html:\n",
    "    grid: \n",
    "      body-width: 1000px\n",
    "      sidebar-width: 300px\n",
    "      margin-width: 300px\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    page-layout: full\n",
    "    code-overflow: wrap\n",
    "    \n",
    "number-sections: true\n",
    "reference-location: margin\n",
    "citation-location: margin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c296f7",
   "metadata": {},
   "source": [
    "# Scraping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e32ac",
   "metadata": {},
   "source": [
    "The old version of the BCAS website doesn't use Java Script animations, so the classic  `BeautifulSoup` library is enough at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c69bda",
   "metadata": {},
   "source": [
    "## Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce6c78",
   "metadata": {},
   "source": [
    "First we should investigate the structure of the website. Article data we are looking for is in the section 'Past Issues' (过刊目录). Each issue page contains links to the full article  in PDF or HTML, and abstracts (摘要) which we will explore next.\n",
    "\n",
    "Our final goal is to retrieve the data on the articles. The website structure shows that the links to the articles can be found inside issues. So, our first move is to get the links to the issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43d58dd-3b9c-4983-a342-9742f1b7e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "url = \"http://old2022.bulletin.cas.cn/zgkxyyk/ch/reader/issue_browser.aspx\"\n",
    "issues = 'data/bcas_issues.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac22097",
   "metadata": {},
   "source": [
    "Let's build a function to extract all issues URLs on the catalogue page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "618d1981-d2f1-4618-85c8-41953b1b7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_urls(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        urls = []\n",
    "        # links to issues have 'a' tag\n",
    "        links = soup.find_all('a')\n",
    "\n",
    "        # extract the href from each 'a'\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "        return urls\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d171d",
   "metadata": {},
   "source": [
    "Once the links are retrieved we need to save them in a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99138a19-2c50-4985-a76e-f0e7bb3455a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_urls(urls, txt):\n",
    "    with open(txt, 'w') as file:\n",
    "        for url in urls:\n",
    "            if url.startswith('issue_list.aspx?year_id='):\n",
    "                file.write(\n",
    "                    'http://old2022.bulletin.cas.cn/zgkxyyk/ch/reader/' + url + '\\n')\n",
    "\n",
    "    print(\"Issues URLs saved to\", txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb7af45",
   "metadata": {},
   "source": [
    "Finally, we can execute the two functions and get the links to the issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a4c64-975f-4119-8ebf-bf0aa0e15bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get URLs\n",
    "urls = get_urls(url)\n",
    "# save to txt\n",
    "save_urls(urls, issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253be5f5",
   "metadata": {},
   "source": [
    "## Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36978163",
   "metadata": {},
   "source": [
    "Once we get the links to the issues, we can itearate through them and retrieve the links to the desired articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c46336",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = 'data/bcas_articles.txt'\n",
    "\n",
    "with open(articles, \"a\") as output:\n",
    "    # read the list of issues pages from issues.txt\n",
    "    with open(issues, \"r\") as file:\n",
    "        issue_urls = file.read().splitlines()\n",
    "\n",
    "    # iterate through each URL\n",
    "    for url in issue_urls:\n",
    "        article_urls = get_urls(url)\n",
    "        if article_urls:\n",
    "            # save article URLs by appending to the file\n",
    "            for article_url in article_urls:\n",
    "                if article_url.startswith('view_abstract.aspx?file_no='):\n",
    "                    output.write(\n",
    "                        'http://old2022.bulletin.cas.cn/zgkxyyk/ch/reader/' + article_url + '\\n')\n",
    "\n",
    "print(\"Article URLs saved to\", articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3016e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "def remove_duplicates(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # remove duplicates while preserving order\n",
    "    unique_lines = []\n",
    "    seen = set()\n",
    "    for line in lines:\n",
    "        if line not in seen:\n",
    "            seen.add(line)\n",
    "            unique_lines.append(line)\n",
    "\n",
    "    # Write the unique lines back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.writelines(unique_lines)\n",
    "\n",
    "    print(f\"Duplicates removed. Check {file_path}\")\n",
    "\n",
    "\n",
    "remove_duplicates(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a4d5f",
   "metadata": {},
   "source": [
    "## Article Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658fc31b",
   "metadata": {},
   "source": [
    "Now we can scrape the data for each BCAS article.\n",
    "To do so we need to analyze the HTML structure of the pages and determine CSS selectors for the desired elements.\n",
    "Article pages contain a lot of data about publications:\n",
    "\n",
    "- Title\n",
    "- Date\n",
    "- Issue\n",
    "- Authors\n",
    "- Affiliations\n",
    "- Abstracts\n",
    "- Keywords\n",
    "- Associated fund projects\n",
    "- Views and downloads statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4511506a",
   "metadata": {},
   "source": [
    "### Define Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18a76c",
   "metadata": {},
   "source": [
    "We can start with a function which gets the desired element using beautifulsoup functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "56ff533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text of an element if it exists\n",
    "def get_element(soup, selector):\n",
    "    element = soup.select_one(selector)\n",
    "    return element.get_text(strip=True, separator=\",\") if element else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b881cc",
   "metadata": {},
   "source": [
    "Next we locate each element through CSS selectors and parse with \"html.parser\". The function returns a dictionary with the extracted data.\n",
    "\n",
    "The parsing process may take some time and connection errors (from both sides) can disturb the process. That's why it is better to wrap parsing in try-except block. If any exception occurs during the process, the function prints an error message with the URL and the specific error, and then returns None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "04b3263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract text using BeautifulSoup\n",
    "def get_data(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # extracting data using CSS selectors\n",
    "        title_cn = get_element(soup, 'span#FileTitle')\n",
    "        title_en = get_element(soup, 'span#EnTitle')\n",
    "        author_cn = get_element(soup, 'div.cn_author')\n",
    "        author_en = get_element(soup, 'div.en_author')\n",
    "        org_cn = get_element(soup, 'div.cn_unit')\n",
    "        org_en = get_element(soup, 'div.en_unit')\n",
    "        abstract_cn = get_element(soup, 'div.zw_zhaiyao')\n",
    "        abstract_en = get_element(soup, 'div.yw_zhaiyao')\n",
    "        keywords_cn = get_element(soup, 'div.zw_gjc')\n",
    "        keywords_en = get_element(soup, 'div.yw_gjc')\n",
    "        fund_project = get_element(soup, 'div.jjxm')\n",
    "        date = get_element(soup, 'div.d_deta.fr')\n",
    "        views = get_element(soup, 'span#ClickNum')\n",
    "        downloads = get_element(soup, 'span#PDFClickNum')\n",
    "\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"title_cn\": title_cn,\n",
    "            \"title_en\": title_en,\n",
    "            \"author_cn\": author_cn,\n",
    "            \"author_en\": author_en,\n",
    "            \"org_cn\": org_cn,\n",
    "            \"org_en\": org_en,\n",
    "            \"abstract_cn\": abstract_cn,\n",
    "            \"abstract_en\": abstract_en,\n",
    "            \"keywords_cn\": keywords_cn,\n",
    "            \"keywords_en\": keywords_en,\n",
    "            \"fund_project\": fund_project,\n",
    "            \"date\": date,\n",
    "            \"views\": views,\n",
    "            \"downloads\": downloads,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4005bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# dataset in csv\n",
    "dataset = \"data/bcas_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae4097",
   "metadata": {},
   "source": [
    "Next we need to werite the scraped data into a csv file. \n",
    "\n",
    "First, we open a csv file in the write mode and define the fieldnames, which are the same as the fields of the dictionary in the get_data() function. We use a pipe \"|\" as a delimeter.\n",
    "\n",
    "Next we open the articles.txt with the URLs and iterate through them retrieving the data, which is tehn stored in the csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "with open(dataset, mode=\"w\", newline=\"\") as csv_file:\n",
    "    fieldnames = [\n",
    "        \"url\", \"date\", \"views\", \"downloads\",\n",
    "        \"author_cn\", \"author_en\",\n",
    "        \"title_cn\", \"title_en\",\n",
    "        \"org_cn\", \"org_en\",\n",
    "        \"abstract_cn\", \"abstract_en\",\n",
    "        \"keywords_cn\", \"keywords_en\",\n",
    "        \"fund_project\"\n",
    "    ]\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames, delimiter=\"|\")\n",
    "    writer.writeheader()\n",
    "\n",
    "    # read the list of URLs from the file\n",
    "    with open(articles, \"r\") as file:\n",
    "        article_urls = file.read().splitlines()\n",
    "\n",
    "        # iterate through each url, extract data, and write to csv\n",
    "        for url in article_urls:\n",
    "            data = get_data(url)\n",
    "            if data:\n",
    "                writer.writerow(data)\n",
    "\n",
    "print(f\"Data has been extracted and saved to {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e39c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "dataset_fixed = \"data/bcas_dataset_fixed.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881f0f4",
   "metadata": {},
   "source": [
    "Due to some formatting issues the parsed data can contain new lines \"/n\" in some cases, breaking the lines. We can fix this issues with a simple fucntion which replaces all line breaks (\\n) in the content with an empty string and then adds a new line break before each occurrence of 'http://old2022.bulletin.cas.cn/'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5730994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix broken lines in the csv\n",
    "def fix_lines(dataset_csv, dataset_csv_fixed):\n",
    "    with open(dataset_csv, mode='r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # replace all line endings with empty string to remove them\n",
    "    content = content.replace('\\n', '')\n",
    "\n",
    "    # add a new line before each 'http' to separate URLs\n",
    "    content = content.replace(\n",
    "        'http://old2022.bulletin.cas.cn/', '\\nhttp://old2022.bulletin.cas.cn/')\n",
    "\n",
    "    with open(dataset_csv_fixed, mode='w', encoding='utf-8') as outfile:\n",
    "        outfile.write(content)\n",
    "\n",
    "\n",
    "fix_lines(dataset, dataset_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00457d3b",
   "metadata": {},
   "source": [
    "### Similar Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3cc742",
   "metadata": {},
   "source": [
    "Our goal is to clasterize the articles using available texts -- so the more useful texts we have, the better. On the pages there is a section called \"Similar Articles\" (相似文献). The titles of similar publications may be useful for topic modeling, for it increases the chances for articles with similar referencies to appear in the same cluster.\n",
    "\n",
    "Accessing this data requires clicking a \"Similar Articles\" button, otherwise the text is not present on the page. We can simulate clicking (and a do lot of other useful stuff) using `selenium` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "with open(similar_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    fieldnames = [\"url\", \"similar\"]\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames, delimiter=\"|\")\n",
    "    writer.writeheader()\n",
    "\n",
    "\n",
    "def initialize_driver():\n",
    "    firefox_options = webdriver.FirefoxOptions()\n",
    "    firefox_options.add_argument(\"--headless\")\n",
    "    return webdriver.Firefox(options=firefox_options)\n",
    "\n",
    "\n",
    "driver = initialize_driver()\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(articles, 'r') as file:\n",
    "    urls = file.read().splitlines()\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable(\n",
    "            (By.XPATH, '//div[text()=\"相似文献\"]'))).click()\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"ArticleList\")))\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # find all <a> with class \"ArticleList\"\n",
    "        links = soup.find_all('a', class_='ArticleList')\n",
    "\n",
    "        # extract the text from each element\n",
    "        similar_list = [link.text.strip() for link in links]\n",
    "\n",
    "    except TimeoutException:\n",
    "        similar_list = ['time_error']\n",
    "\n",
    "    except WebDriverException:\n",
    "        max_retries = 5\n",
    "        retry_count = 0\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                time.sleep(3)\n",
    "                driver.quit()\n",
    "                driver = initialize_driver()\n",
    "                driver.get(url)\n",
    "                WebDriverWait(driver, 2).until(EC.element_to_be_clickable(\n",
    "                    (By.XPATH, '//div[text()=\"相似文献\"]'))).click()\n",
    "                WebDriverWait(driver, 2).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"ArticleList\")))\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                links = soup.find_all('a', class_='ArticleList')\n",
    "                similar_list = [link.text.strip() for link in links]\n",
    "                break\n",
    "\n",
    "            except (WebDriverException, TimeoutException):\n",
    "                retry_count += 1\n",
    "                if retry_count == max_retries:\n",
    "                    similar_list = ['web_error']\n",
    "\n",
    "    finally:\n",
    "        data.append({'url': url, 'similar': similar_list})\n",
    "\n",
    "        with open(similar_csv, 'a', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=[\n",
    "                                    'url', 'similar'], delimiter=\"|\")\n",
    "            writer.writerow({'url': url, 'similar': similar_list})\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "print(f\"Data extracted and scaved to {similar_csv}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e5f8d",
   "metadata": {},
   "source": [
    "### Merge Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6737e9f",
   "metadata": {},
   "source": [
    "Once we have one dataset with article metadata and the other with similar articles we can combine them into one.\n",
    "One way to do so fast and easy is through Pandas.\n",
    "\n",
    "We create two dataframes and merge them through SQL-like function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb5e803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(dataset_fixed, sep='|')\n",
    "similar_df = pd.read_csv(similar_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492a5f3",
   "metadata": {},
   "source": [
    "We can explore the dataset a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2321bafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>views</th>\n",
       "      <th>downloads</th>\n",
       "      <th>author_cn</th>\n",
       "      <th>author_en</th>\n",
       "      <th>title_cn</th>\n",
       "      <th>title_en</th>\n",
       "      <th>org_cn</th>\n",
       "      <th>org_en</th>\n",
       "      <th>abstract_cn</th>\n",
       "      <th>abstract_en</th>\n",
       "      <th>keywords_cn</th>\n",
       "      <th>keywords_en</th>\n",
       "      <th>fund_project</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...</td>\n",
       "      <td>中国科学院院刊:2024,39(1):17-26</td>\n",
       "      <td>470</td>\n",
       "      <td>448</td>\n",
       "      <td>谭光明,,,贾伟乐,,,王展,,,元国军,,,邵恩,,,孙凝晖*</td>\n",
       "      <td>TAN Guangming,,,JIA Weile,,,WANG Zhan,,,YUAN G...</td>\n",
       "      <td>面向模拟智能的计算系统</td>\n",
       "      <td>Computing system for simulation intelligence</td>\n",
       "      <td>(中国科学院计算技术研究所 北京 100190)</td>\n",
       "      <td>(Institute of Computing Technology, Chinese Ac...</td>\n",
       "      <td>中文摘要:,科学研究中的计算机模拟称为科学模拟（scientific simulation）...</td>\n",
       "      <td>Abstract:,This study refers computer simulatio...</td>\n",
       "      <td>中文关键词:,科学模拟,模拟智能,人工智能,计算系统,Z级计算</td>\n",
       "      <td>keywords:,scientific simulation,simulation int...</td>\n",
       "      <td>基金项目:,国家杰出青年科学基金（T2125013）</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "3  http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...   \n",
       "\n",
       "                       date  views  downloads  \\\n",
       "3  中国科学院院刊:2024,39(1):17-26    470        448   \n",
       "\n",
       "                          author_cn  \\\n",
       "3  谭光明,,,贾伟乐,,,王展,,,元国军,,,邵恩,,,孙凝晖*   \n",
       "\n",
       "                                           author_en     title_cn  \\\n",
       "3  TAN Guangming,,,JIA Weile,,,WANG Zhan,,,YUAN G...  面向模拟智能的计算系统   \n",
       "\n",
       "                                       title_en                    org_cn  \\\n",
       "3  Computing system for simulation intelligence  (中国科学院计算技术研究所 北京 100190)   \n",
       "\n",
       "                                              org_en  \\\n",
       "3  (Institute of Computing Technology, Chinese Ac...   \n",
       "\n",
       "                                         abstract_cn  \\\n",
       "3  中文摘要:,科学研究中的计算机模拟称为科学模拟（scientific simulation）...   \n",
       "\n",
       "                                         abstract_en  \\\n",
       "3  Abstract:,This study refers computer simulatio...   \n",
       "\n",
       "                       keywords_cn  \\\n",
       "3  中文关键词:,科学模拟,模拟智能,人工智能,计算系统,Z级计算   \n",
       "\n",
       "                                         keywords_en  \\\n",
       "3  keywords:,scientific simulation,simulation int...   \n",
       "\n",
       "                 fund_project  \n",
       "3  基金项目:,国家杰出青年科学基金（T2125013）  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "# | output: true\n",
    "\n",
    "df.loc[[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86e8fc",
   "metadata": {},
   "source": [
    "And this is what the dataset woth references looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3b16b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>similar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...</td>\n",
       "      <td>['科研信息化发展态势和思考', '数据科学与计算智能：内涵、范式与机遇', '人工智能驱动...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...</td>\n",
       "      <td>['人工智能驱动的科学研究新范式：从AI4S到智能科学', 'GPT技术变革对基础科学研究的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...</td>\n",
       "      <td>['信息化:从计算机科学到计算科学', '科学大数据智能分析软件的现状与趋势', '中国高通...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...</td>\n",
       "      <td>['人工智能驱动的科学研究新范式：从AI4S到智能科学', '适度超前推动科研基础平台建设 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...   \n",
       "1  http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...   \n",
       "2  http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...   \n",
       "3  http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...   \n",
       "4  http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...   \n",
       "\n",
       "                                             similar  \n",
       "0                                                 []  \n",
       "1  ['科研信息化发展态势和思考', '数据科学与计算智能：内涵、范式与机遇', '人工智能驱动...  \n",
       "2  ['人工智能驱动的科学研究新范式：从AI4S到智能科学', 'GPT技术变革对基础科学研究的...  \n",
       "3  ['信息化:从计算机科学到计算科学', '科学大数据智能分析软件的现状与趋势', '中国高通...  \n",
       "4  ['人工智能驱动的科学研究新范式：从AI4S到智能科学', '适度超前推动科研基础平台建设 ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "# | output: true\n",
    "\n",
    "similar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce92b2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "There are multiple ways to combine these two datasets together. Now a SQL-like merge() function will do just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f909cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, similar_df, on='url', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e760e33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>views</th>\n",
       "      <th>downloads</th>\n",
       "      <th>author_cn</th>\n",
       "      <th>author_en</th>\n",
       "      <th>title_cn</th>\n",
       "      <th>title_en</th>\n",
       "      <th>org_cn</th>\n",
       "      <th>org_en</th>\n",
       "      <th>abstract_cn</th>\n",
       "      <th>abstract_en</th>\n",
       "      <th>keywords_cn</th>\n",
       "      <th>keywords_en</th>\n",
       "      <th>fund_project</th>\n",
       "      <th>similar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...</td>\n",
       "      <td>中国科学院院刊:2024,39(1):17-26</td>\n",
       "      <td>470</td>\n",
       "      <td>448</td>\n",
       "      <td>谭光明,,,贾伟乐,,,王展,,,元国军,,,邵恩,,,孙凝晖*</td>\n",
       "      <td>TAN Guangming,,,JIA Weile,,,WANG Zhan,,,YUAN G...</td>\n",
       "      <td>面向模拟智能的计算系统</td>\n",
       "      <td>Computing system for simulation intelligence</td>\n",
       "      <td>(中国科学院计算技术研究所 北京 100190)</td>\n",
       "      <td>(Institute of Computing Technology, Chinese Ac...</td>\n",
       "      <td>中文摘要:,科学研究中的计算机模拟称为科学模拟（scientific simulation）...</td>\n",
       "      <td>Abstract:,This study refers computer simulatio...</td>\n",
       "      <td>中文关键词:,科学模拟,模拟智能,人工智能,计算系统,Z级计算</td>\n",
       "      <td>keywords:,scientific simulation,simulation int...</td>\n",
       "      <td>基金项目:,国家杰出青年科学基金（T2125013）</td>\n",
       "      <td>['信息化:从计算机科学到计算科学', '科学大数据智能分析软件的现状与趋势', '中国高通...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "3  http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...   \n",
       "\n",
       "                       date  views  downloads  \\\n",
       "3  中国科学院院刊:2024,39(1):17-26    470        448   \n",
       "\n",
       "                          author_cn  \\\n",
       "3  谭光明,,,贾伟乐,,,王展,,,元国军,,,邵恩,,,孙凝晖*   \n",
       "\n",
       "                                           author_en     title_cn  \\\n",
       "3  TAN Guangming,,,JIA Weile,,,WANG Zhan,,,YUAN G...  面向模拟智能的计算系统   \n",
       "\n",
       "                                       title_en                    org_cn  \\\n",
       "3  Computing system for simulation intelligence  (中国科学院计算技术研究所 北京 100190)   \n",
       "\n",
       "                                              org_en  \\\n",
       "3  (Institute of Computing Technology, Chinese Ac...   \n",
       "\n",
       "                                         abstract_cn  \\\n",
       "3  中文摘要:,科学研究中的计算机模拟称为科学模拟（scientific simulation）...   \n",
       "\n",
       "                                         abstract_en  \\\n",
       "3  Abstract:,This study refers computer simulatio...   \n",
       "\n",
       "                       keywords_cn  \\\n",
       "3  中文关键词:,科学模拟,模拟智能,人工智能,计算系统,Z级计算   \n",
       "\n",
       "                                         keywords_en  \\\n",
       "3  keywords:,scientific simulation,simulation int...   \n",
       "\n",
       "                 fund_project  \\\n",
       "3  基金项目:,国家杰出青年科学基金（T2125013）   \n",
       "\n",
       "                                             similar  \n",
       "3  ['信息化:从计算机科学到计算科学', '科学大数据智能分析软件的现状与趋势', '中国高通...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "# | output: true\n",
    "df.loc[[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad40bad",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a5f99",
   "metadata": {},
   "source": [
    "The data we got is quite messy with lots of missing values, extra comas and other punctuation signs, some columns can be split into two to make more sense.\n",
    "\n",
    "In this cases we can do some cleaning with `pandas` and regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e154b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "import pandas as pd\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d815a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "dataset_fixed = \"data/bcas_dataset_fixed.csv\"\n",
    "similar_csv = 'data/bcas_similar.csv'\n",
    "\n",
    "df = pd.read_csv(dataset_fixed, sep='|')\n",
    "similar_df = pd.read_csv(similar_csv)\n",
    "\n",
    "df = pd.merge(df, similar_df, on='url', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f79e8d",
   "metadata": {},
   "source": [
    "## Date & Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da68ef0",
   "metadata": {},
   "source": [
    "First, we should transform strings like this '2024,39(1):0-0' into more meaningful form.\n",
    "\n",
    "This string contains three features -- year, issue, and pages. The pattern for separating these features is consistent throughout the dataset, so by a simple split by coma and colon  we can create columns 'date', 'issue', and 'pages'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f249fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中国科学院院刊:2024,39(1):0-0\n",
    "import regex as re\n",
    "\n",
    "df['date'] = df['date'].str.replace('中国科学院院刊:', '', regex=True)\n",
    "df[['date', 'issue']] = df['date'].str.split(',', n=1, expand=True)\n",
    "df[['issue', 'page']] = df['issue'].str.split(':', n=1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5cbc4529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>issue</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>39(1)</td>\n",
       "      <td>0-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>39(1)</td>\n",
       "      <td>1-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>39(1)</td>\n",
       "      <td>10-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>39(1)</td>\n",
       "      <td>17-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>39(1)</td>\n",
       "      <td>27-33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date  issue   page\n",
       "0  2024  39(1)    0-0\n",
       "1  2024  39(1)    1-9\n",
       "2  2024  39(1)  10-16\n",
       "3  2024  39(1)  17-26\n",
       "4  2024  39(1)  27-33"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "# | output: true\n",
    "df.loc[:, ['date', 'issue', 'page']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45a9fb",
   "metadata": {},
   "source": [
    "## Abstracts & Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262e27f",
   "metadata": {},
   "source": [
    "Next we can remove irrelevant words like '中文摘要:', 'Abstract:', '中文关键词:', 'keywords:', '基金项目:'. This can be done through regular expression replacement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8d7b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant text and strip comas\n",
    "df['abstract_cn'] = df['abstract_cn'].str.replace(\n",
    "    '中文摘要:', '', regex=True).str.lstrip(',')\n",
    "df['abstract_en'] = df['abstract_en'].str.replace(\n",
    "    'Abstract:', '', regex=True).str.lstrip(',')\n",
    "df['keywords_cn'] = df['keywords_cn'].str.replace(\n",
    "    '中文关键词:', '', regex=True).str.lstrip(',')\n",
    "df['keywords_en'] = df['keywords_en'].str.replace(\n",
    "    'keywords:', '', regex=True).str.lstrip(',')\n",
    "df['fund_project'] = df['fund_project'].str.replace(\n",
    "    '基金项目:', '', regex=True).str.lstrip(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "084792b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract_cn</th>\n",
       "      <th>abstract_en</th>\n",
       "      <th>keywords_cn</th>\n",
       "      <th>keywords_en</th>\n",
       "      <th>fund_project</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中文摘要:,文章将“智能化科研”（AI4R）称为第五科研范式，概括它的一系列特征包括：（1）...</td>\n",
       "      <td>Abstract:,This article refers to “AI for Resea...</td>\n",
       "      <td>中文关键词:,智能化科研,涌现,组合爆炸问题,非确定计算,大科学模型,科研大平台</td>\n",
       "      <td>keywords:,AI4R,emergence,combinatorial explosi...</td>\n",
       "      <td>基金项目:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         abstract_cn  \\\n",
       "1  中文摘要:,文章将“智能化科研”（AI4R）称为第五科研范式，概括它的一系列特征包括：（1）...   \n",
       "\n",
       "                                         abstract_en  \\\n",
       "1  Abstract:,This article refers to “AI for Resea...   \n",
       "\n",
       "                                keywords_cn  \\\n",
       "1  中文关键词:,智能化科研,涌现,组合爆炸问题,非确定计算,大科学模型,科研大平台   \n",
       "\n",
       "                                         keywords_en fund_project  \n",
       "1  keywords:,AI4R,emergence,combinatorial explosi...        基金项目:  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: true\n",
    "df.loc[[1], ['abstract_cn', 'abstract_en',\n",
    "             'keywords_cn', 'keywords_en', 'fund_project']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba546b0f",
   "metadata": {},
   "source": [
    "## Authors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b3e98",
   "metadata": {},
   "source": [
    "We need to clean the author column as well and remove extra columns, numbers and other symbols like \"*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "849ee768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # replace multiple commas with one\n",
    "    text = re.sub(r',+', ',', text)\n",
    "    # remove asterisks\n",
    "    text = re.sub(r'\\*', '', text)\n",
    "    # remove any leading or trailing commas and whitespace\n",
    "    text = text.strip(',').strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54638905",
   "metadata": {},
   "source": [
    "Some articles do not have a specified name of the author, but each publication is written by a person, so instead of NaN we fill missing values with \"not_specified\" text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "977ae106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_cn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not_specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>李国杰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>鄂维南</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>谭光明,贾伟乐,王展,元国军,邵恩,孙凝晖</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>王飞跃,王雨桐</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author_cn\n",
       "0          not_specified\n",
       "1                    李国杰\n",
       "2                    鄂维南\n",
       "3  谭光明,贾伟乐,王展,元国军,邵恩,孙凝晖\n",
       "4                王飞跃,王雨桐"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "# | output: show\n",
    "df['author_en'] = df['author_en'].fillna('not_specified')\n",
    "df['author_en'] = df['author_en'].apply(normalize_text)\n",
    "df['author_cn'] = df['author_cn'].fillna('not_specified')\n",
    "df['author_cn'] = df['author_cn'].apply(normalize_text)\n",
    "df.loc[:, ['author_cn']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f57488",
   "metadata": {},
   "source": [
    "## Organizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ca4f72",
   "metadata": {},
   "source": [
    "Cleaning organization data is a tricky part, beacuse this part is the most inconsistent and messy.\n",
    "Example of the organization description: (1.北京大学 北京 100871;2.北京科学智能研究院 北京 100084). One articles can be written by several people from different instituitions. The affiliation info also includes data about city, postal codes and job titles in a lot of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a85e87d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = df[['url', 'org_cn']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b14a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace '!' with a space and split on ';'\n",
    "orgs['org_cn'] = orgs['org_cn'].str.replace(\n",
    "    '!', ' ', regex=True).str.split(';')\n",
    "\n",
    "# explode and strip whitespace and parentheses\n",
    "orgs_expld = orgs.explode('org_cn')\n",
    "orgs_expld['org_cn'] = orgs_expld['org_cn'].str.strip().str.strip('()')\n",
    "\n",
    "# remove sequences of 4 to 8 digits at the end and strip again\n",
    "orgs_expld['org_cn'] = orgs_expld['org_cn'].str.replace(\n",
    "    r'\\s*\\d{4,8}\\s*$', ' ', regex=True).str.strip()\n",
    "\n",
    "# remove digits and spaces or commas at any position and leading non-Chinese text followed by a space\n",
    "orgs_expld['org_cn'] = orgs_expld['org_cn'].str.replace(\n",
    "    r'^\\d+\\.\\d*[\\s,]*|\\d+[\\s,]*|^.+?\\s+', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7f07a5",
   "metadata": {},
   "source": [
    "We don't need the postal codes and job titles for our research, but the city data can be useful for geospatial analysis.\n",
    "\n",
    "We can get the relevant cities by checking what city from the list of Chinese cities (get it from Baidu) appears in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "033e1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "city_df = pd.read_csv('data/cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "240821fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a regex from the city list\n",
    "cities_list = city_df['city_cn'].tolist()\n",
    "cities_pattern = r'\\s*(?<=)(\\s*' + \\\n",
    "    '|'.join(map(re.escape, cities_list)) + r')\\b'\n",
    "\n",
    "# compile the regex pattern\n",
    "city_regex = re.compile(cities_pattern)\n",
    "\n",
    "# extract the city from a string\n",
    "\n",
    "\n",
    "def extract_city(text):\n",
    "    match = city_regex.search(text)\n",
    "    if match:\n",
    "        return match.group(0)  # get the match\n",
    "    return None\n",
    "\n",
    "\n",
    "orgs_expld['city_cn'] = orgs_expld['org_cn'].apply(extract_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9266d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "orgs_expld['city_cn'] = orgs_expld['city_cn'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8a76735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "mapping_dict = dict(zip(city_df['city_cn'], city_df['city_en']))\n",
    "orgs_expld['city_en'] = orgs_expld['city_cn'].map(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "606a6973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "orgs_expld['org_cn'] = orgs_expld['org_cn'].apply(lambda x: city_regex.sub('', x).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f802d",
   "metadata": {},
   "source": [
    "Next we need to remove the job titles from the strings.\n",
    "\n",
    "For this purpose we can create a list of job titles which appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e114d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = [\n",
    "    \"所长\", \"研究员\",\n",
    "    \"院长\", \"校长\",\n",
    "    \"主席\", \"总经理\",\n",
    "    \"教授\", \"院士\",\n",
    "    \"博士\", \"学部委员\",\n",
    "    \"委员长\", \"组长\",\n",
    "    \"主任\", \"处长\",\n",
    "    \"部长\", \"主任\",\n",
    "    \"党委书记\", \"秘书\",\n",
    "    \"局长\", \"总裁\",\n",
    "    \"台长\", \"名誉\",\n",
    "    \"特邀顾问\", \"执行\",\n",
    "    \"主管\", \"工程师\",\n",
    "    \"专利代理人\", \"导师\",\n",
    "    \"助理\", \"书记\", \"理事长\", \"馆长\"\n",
    "]\n",
    "\n",
    "job_titles_pattern = '|'.join([fr'\\s*{title}\\s*' for title in job_titles])\n",
    "\n",
    "\n",
    "def remove_job_titles(text):\n",
    "    return re.sub(job_titles_pattern, '', text).strip()\n",
    "\n",
    "\n",
    "orgs_expld['org_cn'] = orgs_expld['org_cn'].apply(remove_job_titles)\n",
    "\n",
    "# remove some characters and 副 from job titles\n",
    "orgs_expld['org_cn'] = orgs_expld['org_cn'].str.replace(\n",
    "    r\"[、《》副]\", \"\", regex=True)\n",
    "# remove empty parentheses\n",
    "orgs_expld['org_cn'] = orgs_expld['org_cn'].str.replace(\n",
    "    r\"\\(\\)\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e5000",
   "metadata": {},
   "source": [
    "Now the strings will look something like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f5199ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>org_cn</th>\n",
       "      <th>city_cn</th>\n",
       "      <th>city_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...</td>\n",
       "      <td>北京大学</td>\n",
       "      <td>北京</td>\n",
       "      <td>Beijing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...</td>\n",
       "      <td>北京科学智能研究院</td>\n",
       "      <td>北京</td>\n",
       "      <td>Beijing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...</td>\n",
       "      <td>中国科学院自动化研究所 复杂系统管理与控制国家重点实验室</td>\n",
       "      <td>北京</td>\n",
       "      <td>Beijing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...</td>\n",
       "      <td>澳门科技大学 创新工程学院</td>\n",
       "      <td>澳门</td>\n",
       "      <td>Macau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...</td>\n",
       "      <td>中国科学院自动化研究所 多模态人工智能系统全国重点实验室</td>\n",
       "      <td>北京</td>\n",
       "      <td>Beijing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "2  http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...   \n",
       "2  http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...   \n",
       "4  http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...   \n",
       "4  http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...   \n",
       "4  http://old2022.bulletin.cas.cn/zgkxyyk/ch/read...   \n",
       "\n",
       "                         org_cn city_cn  city_en  \n",
       "2                          北京大学      北京  Beijing  \n",
       "2                     北京科学智能研究院      北京  Beijing  \n",
       "4  中国科学院自动化研究所 复杂系统管理与控制国家重点实验室      北京  Beijing  \n",
       "4                 澳门科技大学 创新工程学院      澳门    Macau  \n",
       "4  中国科学院自动化研究所 多模态人工智能系统全国重点实验室      北京  Beijing  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "# | output: true\n",
    "orgs_expld = orgs_expld[orgs_expld.org_cn != '']\n",
    "orgs_expld.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29036d34",
   "metadata": {},
   "source": [
    "The next important issue is the degree of detail about organizations we are interested in. The affiliation data includes in some cases the title of the head organization and a subdivision (lab, office, group, ). For now we will focus on the head organizations, e.g. if there is a string like '中国科学院自动化研究所 复杂系统管理与控制国家重点实验室' we count it like '中国科学院自动化研究所'.\n",
    "\n",
    "This also can be achieved through regular exprssions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e24bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of head organizations\n",
    "head_org_endings = [\"科学院\", \"研究所\", \"研究院\", \"大学\", \"学院\"]\n",
    "\n",
    "# Create regex\n",
    "pattern = '|'.join(head_org_endings)\n",
    "\n",
    "# Regex pattern to detect Chinese characters\n",
    "chinese_char_pattern = re.compile(r'[\\u4e00-\\u9fff]')\n",
    "\n",
    "\n",
    "def extract_head_org(text):\n",
    "    if not text.strip():\n",
    "        return text  # Return the original text if it is empty or only whitespace\n",
    "\n",
    "    # Check if the text contains any Chinese characters\n",
    "    if not chinese_char_pattern.search(text):\n",
    "        return text  # Return the original text if it contains no Chinese characters\n",
    "\n",
    "    # Check specifically for \"中国科学院大学\" ignoring other symbols or characters\n",
    "    if re.search(r\"中国科学院大学\", text):\n",
    "        return \"中国科学院大学\"\n",
    "\n",
    "    if \"中国科学院院刊\" in text:\n",
    "        return \"中国科学院院刊\"\n",
    "\n",
    "    if \"上海天文台\" in text:\n",
    "        return \"中国科学院上海天文台\"\n",
    "\n",
    "    if \"北京天文台\" in text:\n",
    "        return \"中国科学院北京天文台\"\n",
    "\n",
    "    if \"南京天文台\" in text:\n",
    "        return \"中国科学院南京天文台\"\n",
    "\n",
    "    if \"国家天文台\" in text:\n",
    "        return \"中国科学院国家天文台\"\n",
    "\n",
    "    # Special case for \"中国科学院\" followed by \"研究所\", \"中心\", or \"研究院\"\n",
    "    zky_with_suffix_match = re.search(r\"(中国科学院.*?(研究所|中心|研究院))\", text)\n",
    "    if zky_with_suffix_match:\n",
    "        return zky_with_suffix_match.group(1)\n",
    "\n",
    "    # Search for the first occurrence of any of the common endings in the full text\n",
    "    match = re.search(fr\"(.+?({pattern}))\", text)\n",
    "    if match:\n",
    "        # Extract and return the head organization\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        # No common ending found, consider the first part as the head organization\n",
    "        return text.split()[0] if ' ' in text else text\n",
    "\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "orgs_expld['org_cn_head'] = orgs_expld['org_cn'].apply(extract_head_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fcd8107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs_head = orgs_expld['org_cn_head'].unique()\n",
    "orgs_head_df = pd.DataFrame(orgs_head, columns=['org_cn_head'])\n",
    "orgs_head_df.to_csv('data/bcas_orgs_head.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64414cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_cn_head</th>\n",
       "      <th>org_cn_head_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>北京师范大学</td>\n",
       "      <td>北京师范大学</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“万种园”项目首席科学家</td>\n",
       "      <td>“万种园”项目</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“中国科学与人文论坛”长</td>\n",
       "      <td>中国科学与人文论坛</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“论坛”处宣传</td>\n",
       "      <td>“论坛”处宣传</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>)中国科学院</td>\n",
       "      <td>中国科学院</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    org_cn_head org_cn_head_clean\n",
       "0        北京师范大学            北京师范大学\n",
       "1  “万种园”项目首席科学家           “万种园”项目\n",
       "2  “中国科学与人文论坛”长         中国科学与人文论坛\n",
       "3       “论坛”处宣传           “论坛”处宣传\n",
       "4        )中国科学院             中国科学院"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "orgs_head_clean_df = pd.read_csv('data/bcas_orgs_head_clean.csv')\n",
    "orgs_head_clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4081f",
   "metadata": {},
   "source": [
    "The next step is to translate the data to English. Note that we need official English titles, so doing everything through machine translation is not the best fit.\n",
    "\n",
    "We can try to retrieve some English titles from Baidu using Beautiful Soup again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e58b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_english_title(org_cn):\n",
    "    base_url = 'https://baike.baidu.com/item/'\n",
    "    url = base_url + org_cn\n",
    "    print(f\"Processing: {org_cn}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # find divs with class \"itemWrapper_yPk3z\" or \"itemWrapper_qgaYJ\"\n",
    "        divs = soup.find_all(\n",
    "            'div', class_=['itemWrapper_yPk3z', 'itemWrapper_qgaYJ'])\n",
    "        for div in divs:\n",
    "            # find dt with \"外文名\"\n",
    "            dt = div.find('dt', class_='basicInfoItem_hdTH0 itemName_iCg2R')\n",
    "            if dt and '外文名' in dt.text:\n",
    "                # find the dd element with class \"basicInfoItem_hdTH0 itemValue_rxziX\"\n",
    "                dd = div.find(\n",
    "                    'dd', class_='basicInfoItem_hdTH0 itemValue_rxziX')\n",
    "                if dd:\n",
    "                    # extract English title\n",
    "                    span = dd.find('span', class_='text_v1llE')\n",
    "                    if span:\n",
    "                        english_title = span.text.strip()\n",
    "                        print(\n",
    "                            f\"Found English title for {org_cn}: {english_title}\")\n",
    "                        return english_title\n",
    "        print(f\"No English title found for {org_cn}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {org_cn}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "orgs_head_clean_df['org_cn_head_en'] = orgs_head_clean_df['org_cn_head_clean'].apply(\n",
    "    fetch_english_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fdd84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "orgs_head_clean_df.to_csv('data/bcas_orgs_head_clean_en.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb3dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1259 entries, 0 to 1258\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   org_cn_head        1259 non-null   object\n",
      " 1   org_cn_head_clean  1259 non-null   object\n",
      " 2   org_cn_head_en     600 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 29.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: false\n",
    "orgs_head_clean_df[orgs_head_clean_df['org_cn_head_en'] == '']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706262e",
   "metadata": {},
   "source": [
    "Unfortunately, not all organizations are present on Baidu or have English translation. We can translate these with Google Translate through `deep_translator` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5401b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 659/659 [19:41<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_translation(text):\n",
    "    try:\n",
    "        return GoogleTranslator(source='auto', target='en').translate(str(text))\n",
    "    except KeyboardInterrupt as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {text}. Error: {e}\")\n",
    "        return 'error'\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# translate only the rows where 'org_cn_head_en' is null\n",
    "orgs_head_clean_df.loc[orgs_head_clean_df['org_cn_head_en'].isnull(\n",
    "), 'org_cn_head_en'] = orgs_head_clean_df.loc[orgs_head_clean_df['org_cn_head_en'].isnull(), 'org_cn_head_clean'].progress_apply(get_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829514e",
   "metadata": {},
   "source": [
    "This way we can get the translation of all organization titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94dc376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_cn_head</th>\n",
       "      <th>org_cn_head_clean</th>\n",
       "      <th>org_cn_head_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>北京师范大学</td>\n",
       "      <td>北京师范大学</td>\n",
       "      <td>Beijing Normal University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“万种园”项目首席科学家</td>\n",
       "      <td>“万种园”项目</td>\n",
       "      <td>\"Ten Thousand Plants Garden\" Project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“中国科学与人文论坛”长</td>\n",
       "      <td>中国科学与人文论坛</td>\n",
       "      <td>China Science and Humanities Forum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“论坛”处宣传</td>\n",
       "      <td>“论坛”处宣传</td>\n",
       "      <td>Publicity at the Forum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>)中国科学院</td>\n",
       "      <td>中国科学院</td>\n",
       "      <td>Chinese Academy of Sciences</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    org_cn_head org_cn_head_clean                        org_cn_head_en\n",
       "0        北京师范大学            北京师范大学             Beijing Normal University\n",
       "1  “万种园”项目首席科学家           “万种园”项目  \"Ten Thousand Plants Garden\" Project\n",
       "2  “中国科学与人文论坛”长         中国科学与人文论坛    China Science and Humanities Forum\n",
       "3       “论坛”处宣传           “论坛”处宣传                Publicity at the Forum\n",
       "4        )中国科学院             中国科学院           Chinese Academy of Sciences"
      ]
     },
     "execution_count": 1169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "orgs_head_clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45445c9",
   "metadata": {},
   "source": [
    "We add some final touches in Excel/Google Sheets and update the dataframe. This dataset is small, so it is faster to just manually remove irrelevant symbols than to craft automated solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953d853b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_bf881\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_bf881_level0_col0\" class=\"col_heading level0 col0\" >Head Organization</th>\n",
       "      <th id=\"T_bf881_level0_col1\" class=\"col_heading level0 col1\" >Head Organization Cleaned</th>\n",
       "      <th id=\"T_bf881_level0_col2\" class=\"col_heading level0 col2\" >Head Organization English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_bf881_row0_col0\" class=\"data row0 col0\" >北京师范大学</td>\n",
       "      <td id=\"T_bf881_row0_col1\" class=\"data row0 col1\" >北京师范大学</td>\n",
       "      <td id=\"T_bf881_row0_col2\" class=\"data row0 col2\" >Beijing Normal University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bf881_row1_col0\" class=\"data row1 col0\" >“万种园”项目首席科学家</td>\n",
       "      <td id=\"T_bf881_row1_col1\" class=\"data row1 col1\" >“万种园”项目</td>\n",
       "      <td id=\"T_bf881_row1_col2\" class=\"data row1 col2\" >\"Ten Thousand Plants Garden\" Project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bf881_row2_col0\" class=\"data row2 col0\" >“中国科学与人文论坛”长</td>\n",
       "      <td id=\"T_bf881_row2_col1\" class=\"data row2 col1\" >中国科学与人文论坛</td>\n",
       "      <td id=\"T_bf881_row2_col2\" class=\"data row2 col2\" >China Science and Humanities Forum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bf881_row3_col0\" class=\"data row3 col0\" >“论坛”处宣传</td>\n",
       "      <td id=\"T_bf881_row3_col1\" class=\"data row3 col1\" >“论坛”处宣传</td>\n",
       "      <td id=\"T_bf881_row3_col2\" class=\"data row3 col2\" >Publicity at the Forum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_bf881_row4_col0\" class=\"data row4 col0\" >)中国科学院</td>\n",
       "      <td id=\"T_bf881_row4_col1\" class=\"data row4 col1\" >中国科学院</td>\n",
       "      <td id=\"T_bf881_row4_col2\" class=\"data row4 col2\" >CAS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x133e3b0b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "# | output: true\n",
    "orgs_head_clean_df = pd.read_csv('data/bcas_orgs_head_clean_en_upd_v2.csv')\n",
    "orgs_head_clean_df.head().style.hide(axis=\"index\").relabel_index([\"Head Organization\", \"Head Organization Cleaned\", \"Head Organization English\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aefa48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "mapping_dict = dict(\n",
    "    zip(orgs_head_clean_df['org_cn_head'], orgs_head_clean_df['org_cn_head_en']))\n",
    "orgs_expld['org_cn_head_en'] = orgs_expld['org_cn_head'].map(mapping_dict)\n",
    "\n",
    "city_cn_mapping = orgs_expld.dropna(subset=['city_cn']).drop_duplicates(\n",
    "    subset=['org_cn_head_en']).set_index('org_cn_head_en')['city_cn'].to_dict()\n",
    "city_en_mapping = orgs_expld.dropna(subset=['city_en']).drop_duplicates(\n",
    "    subset=['org_cn_head_en']).set_index('org_cn_head_en')['city_en'].to_dict()\n",
    "\n",
    "orgs_expld['city_cn'] = orgs_expld.apply(\n",
    "    lambda row: city_cn_mapping.get(row['org_cn_head_en'], row['city_cn']), axis=1)\n",
    "orgs_expld['city_en'] = orgs_expld.apply(\n",
    "    lambda row: city_en_mapping.get(row['org_cn_head_en'], row['city_en']), axis=1)\n",
    "\n",
    "title_dict = df.set_index('url')['title_cn'].to_dict()\n",
    "orgs_expld['title_cn'] = orgs_expld['url'].map(title_dict)\n",
    "\n",
    "year_dict = df.set_index('url')['date'].to_dict()\n",
    "orgs_expld['year'] = orgs_expld['url'].map(year_dict)\n",
    "\n",
    "orgs_expld = orgs_expld.rename(columns={'org_cn_head_en': 'orgs_head'})\n",
    "orgs_expld['orgs_head'] = orgs_expld['orgs_head'].str.title()\n",
    "orgs_expld['orgs_head'] = orgs_expld['orgs_head'].str.replace(\n",
    "    'Cas', 'CAS', regex=True)\n",
    "orgs_expld['orgs_head'] = orgs_expld['orgs_head'].str.replace(\n",
    "    'Of', 'of', regex=True)\n",
    "orgs_expld['orgs_head'] = orgs_expld['orgs_head'].str.replace(\n",
    "    'And', 'and', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa42907",
   "metadata": {},
   "source": [
    "Finally, we map years, cities and article titles to the dataframe and format the titles (change case, add abbreviations). The final result will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f094940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ac0bd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_ac0bd_level0_col0\" class=\"col_heading level0 col0\" >Organization</th>\n",
       "      <th id=\"T_ac0bd_level0_col1\" class=\"col_heading level0 col1\" >Head Organization (CN)</th>\n",
       "      <th id=\"T_ac0bd_level0_col2\" class=\"col_heading level0 col2\" >Head Organization (EN)</th>\n",
       "      <th id=\"T_ac0bd_level0_col3\" class=\"col_heading level0 col3\" >City (EN)</th>\n",
       "      <th id=\"T_ac0bd_level0_col4\" class=\"col_heading level0 col4\" >City (CN)</th>\n",
       "      <th id=\"T_ac0bd_level0_col5\" class=\"col_heading level0 col5\" >Article</th>\n",
       "      <th id=\"T_ac0bd_level0_col6\" class=\"col_heading level0 col6\" >Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_ac0bd_row0_col0\" class=\"data row0 col0\" >北京大学</td>\n",
       "      <td id=\"T_ac0bd_row0_col1\" class=\"data row0 col1\" >北京大学</td>\n",
       "      <td id=\"T_ac0bd_row0_col2\" class=\"data row0 col2\" >Peking University</td>\n",
       "      <td id=\"T_ac0bd_row0_col3\" class=\"data row0 col3\" >北京</td>\n",
       "      <td id=\"T_ac0bd_row0_col4\" class=\"data row0 col4\" >Beijing</td>\n",
       "      <td id=\"T_ac0bd_row0_col5\" class=\"data row0 col5\" >AI助力打造科学研究新范式</td>\n",
       "      <td id=\"T_ac0bd_row0_col6\" class=\"data row0 col6\" >2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ac0bd_row1_col0\" class=\"data row1 col0\" >北京科学智能研究院</td>\n",
       "      <td id=\"T_ac0bd_row1_col1\" class=\"data row1 col1\" >北京科学智能研究院</td>\n",
       "      <td id=\"T_ac0bd_row1_col2\" class=\"data row1 col2\" >Beijing Institute of Scientific and Intelligent Technology</td>\n",
       "      <td id=\"T_ac0bd_row1_col3\" class=\"data row1 col3\" >北京</td>\n",
       "      <td id=\"T_ac0bd_row1_col4\" class=\"data row1 col4\" >Beijing</td>\n",
       "      <td id=\"T_ac0bd_row1_col5\" class=\"data row1 col5\" >AI助力打造科学研究新范式</td>\n",
       "      <td id=\"T_ac0bd_row1_col6\" class=\"data row1 col6\" >2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ac0bd_row2_col0\" class=\"data row2 col0\" >中国科学院自动化研究所 复杂系统管理与控制国家重点实验室</td>\n",
       "      <td id=\"T_ac0bd_row2_col1\" class=\"data row2 col1\" >中国科学院自动化研究所</td>\n",
       "      <td id=\"T_ac0bd_row2_col2\" class=\"data row2 col2\" >Institute of Automation, CAS</td>\n",
       "      <td id=\"T_ac0bd_row2_col3\" class=\"data row2 col3\" >北京</td>\n",
       "      <td id=\"T_ac0bd_row2_col4\" class=\"data row2 col4\" >Beijing</td>\n",
       "      <td id=\"T_ac0bd_row2_col5\" class=\"data row2 col5\" >数字科学家与平行科学：AI4S和S4AI的本源与目标</td>\n",
       "      <td id=\"T_ac0bd_row2_col6\" class=\"data row2 col6\" >2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ac0bd_row3_col0\" class=\"data row3 col0\" >澳门科技大学 创新工程学院</td>\n",
       "      <td id=\"T_ac0bd_row3_col1\" class=\"data row3 col1\" >澳门科技大学</td>\n",
       "      <td id=\"T_ac0bd_row3_col2\" class=\"data row3 col2\" >Macau University of Science and Technology</td>\n",
       "      <td id=\"T_ac0bd_row3_col3\" class=\"data row3 col3\" >澳门</td>\n",
       "      <td id=\"T_ac0bd_row3_col4\" class=\"data row3 col4\" >Macau</td>\n",
       "      <td id=\"T_ac0bd_row3_col5\" class=\"data row3 col5\" >数字科学家与平行科学：AI4S和S4AI的本源与目标</td>\n",
       "      <td id=\"T_ac0bd_row3_col6\" class=\"data row3 col6\" >2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ac0bd_row4_col0\" class=\"data row4 col0\" >中国科学院自动化研究所 多模态人工智能系统全国重点实验室</td>\n",
       "      <td id=\"T_ac0bd_row4_col1\" class=\"data row4 col1\" >中国科学院自动化研究所</td>\n",
       "      <td id=\"T_ac0bd_row4_col2\" class=\"data row4 col2\" >Institute of Automation, CAS</td>\n",
       "      <td id=\"T_ac0bd_row4_col3\" class=\"data row4 col3\" >北京</td>\n",
       "      <td id=\"T_ac0bd_row4_col4\" class=\"data row4 col4\" >Beijing</td>\n",
       "      <td id=\"T_ac0bd_row4_col5\" class=\"data row4 col5\" >数字科学家与平行科学：AI4S和S4AI的本源与目标</td>\n",
       "      <td id=\"T_ac0bd_row4_col6\" class=\"data row4 col6\" >2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1742fffb0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "# | output: true\n",
    "orgs_expld = pd.read_csv('data/bcas_orgs.csv')\n",
    "orgs_expld = orgs_expld[['org_cn', 'org_cn_head', 'orgs_head', 'city_cn', 'city_en', 'title_cn', 'year']]\n",
    "orgs_expld.head(5).style.hide(axis=\"index\").relabel_index([\"Organization\", \"Head Organization (CN)\", \"Head Organization (EN)\", \"City (EN)\", \"City (CN)\", \"Article\", \"Year\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc734a9",
   "metadata": {},
   "source": [
    "## Fund Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48416d07",
   "metadata": {},
   "source": [
    "Finally, we need to clean the data about associated fund projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118c612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7216, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "\n",
    "fund = df[['url', 'date', 'title_cn', 'fund_project']]\n",
    "fund.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7794dc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1054, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "\n",
    "fund = fund[fund['fund_project'].str.strip() != '']\n",
    "fund.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83bea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fund['fund_project'] = fund['fund_project'].str.replace('基金项目：', '', regex=True)\n",
    "fund['fund_project'] = (fund['fund_project'].str.replace(',', '，', regex=False)\n",
    "                                            .str.replace(';', '，', regex=False)\n",
    "                                            .str.replace(r'\\(', '（', regex=True)\n",
    "                                            .str.replace(r'\\)', '）', regex=True)\n",
    "                                            .str.replace('!', '，', regex=False)\n",
    "                                            .str.replace('；', '，', regex=False)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b809b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_commas_in_brackets(text):\n",
    "    return re.sub(r'[（(](.*?)[）)]', lambda m: re.sub(r'[!,;，；]', '、', m.group()), text)\n",
    "\n",
    "fund['fund_project'] = fund['fund_project'].apply(replace_commas_in_brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813671ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "\n",
    "fund['fund_project'] = fund['fund_project'].str.split('，')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca97a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "\n",
    "fund_expld = fund.explode('fund_project').reset_index()\n",
    "fund_expld['fund_project'] = fund_expld['fund_project'].str.replace('）、', '），')\n",
    "fund_expld['fund_project'] = fund_expld['fund_project'].str.split('，')\n",
    "fund_expld = fund_expld.explode('fund_project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c199c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "\n",
    "# drop irrelevant rows\n",
    "fund_expld = fund_expld.drop(index=[1643] + list(range(1654, 1667)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e85ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "\n",
    "fund_expld.to_csv('data/bcas_fund_projects.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246c04e",
   "metadata": {},
   "source": [
    "The final dataframe looks this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "94b5c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "fund_expld = fund_expld.drop(index=[1645, 1646, 1839])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6e4ec92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# | output: false\n",
    "fund_expld = fund_expld[['date', 'title_cn', 'fund_project']]\n",
    "fund_expld.to_csv('data/bcas_fund_projects_fin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bd3a93b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_9b98e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_9b98e_level0_col0\" class=\"col_heading level0 col0\" >Year</th>\n",
       "      <th id=\"T_9b98e_level0_col1\" class=\"col_heading level0 col1\" >Article</th>\n",
       "      <th id=\"T_9b98e_level0_col2\" class=\"col_heading level0 col2\" >Fund Project</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_9b98e_row0_col0\" class=\"data row0 col0\" >2024</td>\n",
       "      <td id=\"T_9b98e_row0_col1\" class=\"data row0 col1\" >面向模拟智能的计算系统</td>\n",
       "      <td id=\"T_9b98e_row0_col2\" class=\"data row0 col2\" >国家杰出青年科学基金（T2125013）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9b98e_row1_col0\" class=\"data row1 col0\" >2024</td>\n",
       "      <td id=\"T_9b98e_row1_col1\" class=\"data row1 col1\" >数字科学家与平行科学：AI4S和S4AI的本源与目标</td>\n",
       "      <td id=\"T_9b98e_row1_col2\" class=\"data row1 col2\" >澳门科学技术发展基金（0093/2023/RIA2）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9b98e_row2_col0\" class=\"data row2 col0\" >2024</td>\n",
       "      <td id=\"T_9b98e_row2_col1\" class=\"data row2 col1\" >数字科学家与平行科学：AI4S和S4AI的本源与目标</td>\n",
       "      <td id=\"T_9b98e_row2_col2\" class=\"data row2 col2\" >国家自然科学基金（61533019）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9b98e_row3_col0\" class=\"data row3 col0\" >2024</td>\n",
       "      <td id=\"T_9b98e_row3_col1\" class=\"data row3 col1\" >AI for Technology：技术智能在高技术领域的应用实践与未来展望</td>\n",
       "      <td id=\"T_9b98e_row3_col2\" class=\"data row3 col2\" >国家自然科学基金（61925208、U22A2028）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9b98e_row4_col0\" class=\"data row4 col0\" >2024</td>\n",
       "      <td id=\"T_9b98e_row4_col1\" class=\"data row4 col1\" >AI for Technology：技术智能在高技术领域的应用实践与未来展望</td>\n",
       "      <td id=\"T_9b98e_row4_col2\" class=\"data row4 col2\" >中国科学院稳定支持基础研究领域青年团队计划（YSBR-029）</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17789b500>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | echo: false\n",
    "# | output: true\n",
    "fund_expld.head().style.hide(axis=\"index\").relabel_index([\"Year\", \"Article\", \"Fund Project\"], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
