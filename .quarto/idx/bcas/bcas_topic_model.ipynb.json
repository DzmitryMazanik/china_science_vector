{"title":"Building Topic Model","markdown":{"yaml":{"title":"Building Topic Model","jupyter":"python3","format":{"html":{"grid":{"body-width":"1000px","sidebar-width":"300px","margin-width":"300px"},"toc":true,"toc-title":"Contents","page-layout":"full","code-overflow":"wrap"}},"number-sections":true,"reference-location":"margin","citation-location":"margin"},"headingText":"Building BERTopic Model","containsRefs":false,"markdown":"\n\nOur next step focuses on gaining a deeper understanding of the research topics covered in BCAS publications. Positioned as a strategic S&T think tank, it is essential to explore its strategic focuses and how these have evolved over the past 36 years.\n\nTo answer these questions, we will construct a topic model using the dataset. For this purpose, we need to ensure we have the appropriate text data. The dataset includes article titles, keywords, and abstracts, but it also contains significant gaps: some articles have only titles, while others lack keywords or abstracts entirely.\n\nMoreover, since our primary interest lies in the technologies discussed, we must filter out publications that may introduce noise and complicate the interpretation of the topics. We will exclude the following irrelevant publications:\n\n- Covers (封面)\n- Tables of contents (目录)\n- Policy digests (政策速览)\n- Advertising (《中国科学院院刊》新媒体)\n- Prefaces (序言)\n- Technology progress digests lacking abstracts (科研进展)\n- Calls for papers\n- Blank entries\n\nThese publications do not contribute to a deeper understanding of the underlying topics and only serve to create noise.\n\nPreviously we have scraped data on similar articles. This data provides us with a form of clustering that we can leverage in our analysis. To summarize, the texts we will utilize for topic modeling will consist of a combination of **titles, keywords, abstracts, and references to similar articles**.\n\n***\n\nThere are various kinds of topics modeling techniques available. One of the older NLP techniques is Latent Dirichlet Allocation[^1], which was introduced in 2003 but is still widely used with multiple implementations in Python and R. We will use `BERTopic` library[^2], which\n\n> ... uses transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. \n\n\n[^1]: You can find an example of LDA implementation for Chinese using `gensim` in the code notebook.\n[^2]: See documentation [here](https://maartengr.github.io/BERTopic/index.html)\n\n\n## Data Preprocessing\n\nTo prepare text for modeling with BERTopic, we first need to tokenize the text. While standard tokenization methods involve splitting words by whitespace, this approach is unsuitable for Chinese. Instead, we’ll use `jieba` to effectively tokenize the text, allowing us to create a `CountVectorizer` alternative tailored for Chinese.\n\nAlthough the BERTopic documentation suggests that stopword removal is often unnecessary, I found that including stopwords significantly increased noise in the model. Therefore, we'll be incorporating a stopword list using the `cntext` library[^3] to improve the quality of the results.\n\nNext, we will write a function to tokenize text and remove stopwords.\n\n[^3]: For more details, see the Chinese documentation [here](https://github.com/hidadeng/cntext) and the English version [here](https://github.com/hiDaDeng/cntext/blob/main/english_readme.md).\n\nNext, we proceed to fitting the model. BERTopic captures semantic similarities by first transforming the input documents into numerical representations, known as embeddings. Typically, BERTopic leverages models from the sentence-transformers library; for our analysis, we employ the \"paraphrase-multilingual-MiniLM-L12-v2\" model, which is well-suited for capturing nuanced meanings across multiple languages.\n\nA critical decision point is determining the number of topics. While there are technical methods to estimate the optimal number of clusters, practical experience suggests that alignment with the research objective is paramount. We experimented with various models, exploring topic counts of 30, 40, 50, and 80. Fewer topics provide a broad overview, making it easier to identify overarching themes. However, this approach often leaves room for further topic refinement. Given that our goal is to discern specific technological trends, we require a higher level of detail.\n\nAfter several iterations, I determined that 100 topics strike the best balance between granularity and comprehensibility. In the code, we specify 101 topics, reserving one cluster for outliers to ensure that less relevant data does not skew the results. This approach allows us to capture a more nuanced and detailed picture of the technological landscape we are studying.\n\nOnce the model is fitted, we can begin examining the results. BERTopic generates a topic label by identifying the most relevant words within each topic, providing a concise representation of the underlying theme. This label serves as a quick summary, capturing the essence of the topic based on the most significant terms.\n\n## Analyzing Topics\n\nOnce we've identified the topics, we can begin a deeper analysis to uncover the distribution and significance of each. The most represented topics provide key insights into the dominant themes within the dataset. Among these, the most frequent topics include:\n\n1. **Innovation, Knowledge, Cooperation, Technology** (0_创新_知识_合作_科技)\n2. **Year, Introduction, Laboratory, Chinese Academy of Sciences** (1_年度_简介_实验室_中国科学院)\n3. **Innovation, Technology, Major Power, Reform** (2_创新_科技_强国_改革)\n4. **Biology, Organism, Cell, Gene** (3_生物学_生物_细胞_基因)\n5. **Wetland, Water Resources, Yangtze, Ecology** (4_湿地_水资源_长江_生态)\n\nThese topics highlight the emphasis on innovation, scientific research, and environmental concerns, reflecting the strategic focus areas of the publications.\n\nHowever, a significant portion of the documents—nearly 37%—fall into the outliers. This indicates a substantial number of articles that do not neatly fit into the defined topics, which could suggest a wide range of unique or less common subjects being covered. While this diversity is valuable, the high percentage of outliers also suggests that there may be a considerable amount of content that is either very specific or not directly aligned with the main thematic clusters.\n\nTo gain a deeper understanding of the topics generated by BERTopic, we can examine the topic word scores, which are based on cTF-IDF (class-based Term Frequency-Inverse Document Frequency) values. These scores highlight the words that are most representative and carry the most weight within each topic, giving us insights into the core themes of each cluster. By analyzing these topic word scores, we can form hypotheses about the central themes and the specific aspects of technology and innovation that each topic addresses.\n\nFor example, in Topic 0, the words with the highest cTF-IDF values are \"innovation,\" \"knowledge,\" \"cooperation,\" \"S&T\" (Science and Technology), and \"talent.\" These keywords suggest that the documents in this cluster likely focus on issues related to scientific and technological innovation. The presence of words like \"cooperation\" and \"talent\" indicates that the discussion may also involve the human and collaborative aspects of fostering innovation. This could include topics such as the collaboration between scientists within China and internationally, the development of talent necessary for advancing innovation, and the exchange of knowledge that drives scientific progress. The focus on collaboration within the topic related to \"innovation,\" \"knowledge,\" \"cooperation,\" \"S&T,\" and \"talent\" appears to be in contrast with the recent political direction in China, which emphasizes a more isolationist stance. This divergence raises interesting questions about the alignment between the strategic goals in the scientific community and the broader political climate.\n\nThe representation of Topic 1 with terms such as 'annual,' 'introduction,' 'laboratory,' 'Chinese Academy of Sciences,' and 'first prize' indicates that this cluster primarily focuses on the internal workings and institutional highlights of CAS. The presence of words like 'Chinese Academy of Sciences' and 'laboratory' suggests that the documents in this topic are centered around the internal operations of CAS. This could include details about new developments, research facilities, and organizational changes. 'Annual' points to documents that likely cover yearly reports or summaries. Terms like 'introduction' and 'first prize' imply that the documents might also discuss new leadership introductions, significant awards, or recognitions received by the academy. This cluster may reflect a focus on internal communications and documentation that serve to update stakeholders about the academy's progress, innovations, and administrative changes.\n\nBERTopic provides a functionality for visualizing the relationships between topics using an intertopic distance map. This map displays how closely related various topics are to one another, often revealing clusters of topics that share similar themes or concepts. In our case, the intertopic distance map suggests that many topics are closely aligned and could potentially be grouped into larger clusters.\n\nHowever, for the purposes of our project, which aims to uncover the specific technological focus of the publications, a more fine-grained approach is essential. By maintaining a higher number of distinct topics, we can better identify subtle differences and emerging trends within the technological landscape. This granularity allows us to capture the nuances in the research and development areas, providing a clearer picture of the strategic directions and innovation priorities reflected in the publications.\n\nWe can also observe the connections between topics using hierarchical clustering visualization.\n\nWhen we look at the topics, we see that some of them have overlapping keywords. The similarity matrix shows that a few topics are pretty similar. For example, the topics labeled 'innovation, knowledge, cooperation, S&T' (0_创新_知识_合作_科技) and 'innovation, S&T, major power, reform' (2_创新_科技_强国_改革) have high similarity scores. But there’s an important difference between them:\n\n1. **Topic 0 (创新_知识_合作_科技)**: This one is all about how innovation ties in with knowledge and collaboration. It’s focused on how scientific and tech progress connects with spreading knowledge and working together with other researchers.\n\n2. **Topic 2 (创新_科技_强国_改革)**: On the other hand, this topic is about the innovation process in the context of making China a major player in science and tech, along with the necessary reforms. It’s more about the political and strategic side of innovation, aimed at boosting the country’s overall strength and driving big changes.\n\nSo, even though both topics deal with innovation, Topic 0 is more about the practical aspects of knowledge and teamwork, while Topic 2 is more about the strategic and political side of making China a global leader in science and technology.\n\nThis shows that even if topics share some keywords and have high cosine similarity score, they can have very different focuses and themes.\n\nThis is exactly why I decided not to merge the topics. I want to avoid adding any more bias to the analysis. Keeping the topics separate helps us get a clearer, more nuanced view of the different themes and perspectives in the publications.\n\nBut we still need to deal with the outliers.\n\n## Reducing Outliers\n\nArticles often cover multiple themes, especially when they’re interdisciplinary. To handle this, we calculate the probability of each topic for every document. For this we need to set a probability threshold. This threshold helps us decide which topics are relevant enough to be included for each document, filtering out those with lower probabilities that might not add much value.[^4]\n\nBy using this method, we’ve managed to cut down the number of outliers from 36% to just 6%. We also end up with multiple topics assigned to each document, reflecting the real-world complexity and interdisciplinarity of strategic articles. This approach provides a more nuanced view of the content, aligning closely with how strategic topics often span across various fields. Using this method, we can avoid the hassle of merging topics manually since similar topics will naturally be assigned to the same documents.\n\n[^4]: We took some practical ideas on this matter from a great article [Topics per Class Using BERTopic](https://towardsdatascience.com/topics-per-class-using-bertopic-252314f2640) by Mariya Mansurova.  \n\n# Topic Interpretation\n\nThe original BERTopic labels give us a starting point, but to truly grasp their meaning, we need to translate these labels into more understandable terms.\n\nFor this, we can use advanced Large Language Models (LLMs), which excel at text processing. These models are great at predicting the next most likely word, which can help us craft clear and concise summaries of the topics.\n\nGoogle offers robust functionality with its Gemini-family models through AI Studio, and it's quite accessible (free, actually). All we need to do is generate an API key and connect to Gemini using the `google.generativeai` library. This will allow us to enhance our topic interpretations with a bit of AI assistance.\n\n## Prompt Engineering\n\nPrompt engineering is essential when working with Large Language Models (LLMs). To get the most accurate and useful results, we can craft our prompts carefully. \n\nBased on the advice from BERTopic developer Maarten Grootendorst, we can start with a prompt like this:\n\n> I have a topic that is described by the following keywords: [KEYWORDS]\n> In this topic, the following documents are a small but representative subset of all documents in the topic: [DOCUMENTS]\n> Based on the information above, please provide a description of this topic in the following format: topic: <description>\n\nThis method summarizes the most representative documents to generate a topic description. We’ll tweak this slightly to make the most of Gemini’s large context window by including all relevant documents (article titles) instead of just a subset. Additionally, we need to ensure that the descriptions are generated in both Chinese and English.\n\n## Summarizing Topics\n\nOnce we’ve crafted the prompt, the next step is to iterate through each topic and generate its summary.\n\nAlthough LLMs offer significant assistance, we still need to evaluate the topics ourselves.[^5] Some of the labels generated by Gemini may include excessive details or might be too similar to one another. Here is the updated list of topic labels:\n\n[^5]: The best option would be to consult experts from relevant science fields.\n\n# Classification by Science-Metrix Ontology\n\nNow that we have BCAS publications clustered into 100 distinct topics, we might find it helpful to step back and get a broader view. To do this, we can introduce an additional layer of metadata by classifying these topics into scientific fields.\n\nFor this purpose, we’ll leverage the ontology developed by [Science-Metrix](https://www.science-metrix.com/classification/). This classification system sorts scientific journals and articles into 5 domains, 20 fields, and 174 mutually exclusive subfields. Created in 2010 for the European Commission, this system was designed to address the absence of a universal classification scheme within the bibliometric community. It builds on existing journal classifications and was refined through a blend of algorithmic methods and expert judgment. In 2019, the classification was further enhanced:\n\n>  ... we further developed the journal-based classification into the article-level and hybrid versions. For the article-level classification, a scientific publication is attributed to a domain, field and subfield based on its title, abstract, keywords, author affiliation and the classification of its citations, using a deep neural network (an artificial intelligence technique). In the hybrid version, most articles are still classified at the journal level, except for those published in multidisciplinary journals (e.g., Science, Nature, PNAS and PLOS One), which are classified at the article level.\n\nApplying this classification system allows us to map our 100 topics to a structured framework, providing a clearer overview and uncovering broader trends. This approach offers flexibility, letting us choose different levels of granularity—whether analyzing data at the topic, subfield, field, or domain level.\n\n[^6]: Excel file with classification can be dowloaded [here](https://www.science-metrix.com/classification/).\n\nWe can use Gemini for the classification task as well! By using the English topics, we can prompt Gemini to identify the most appropriate subfield for each topic. The complete script, along with the prompt, looks like this:\n\nThe Gemini classification is pretty decent, but it’s not perfect—manual review and tweaking are still necessary. Here's an example of how we match topics to their most fitting subfields:\n\nNext we just need to match subfields with associated fields and domains. \n\n## Summary\n\nIn this chapter, we developed a topic model using the `BERTopic` library, opting not to manually merge topics. Instead, we utilized the Science-Metrix ontology, with assistance from Generative AI, for classification. This strategy allows us to maintain flexibility in our analysis, offering multi-level insights depending on the desired level of granularity. By assigning multiple topics to the BCAS articles, our approach more accurately reflects real-world scenarios, particularly in the context of S&T strategic documents where interdisciplinarity is often the norm.\n\nOur next step is to dive into the data using this new semantic layer.\n","srcMarkdownNoYaml":"\n\nOur next step focuses on gaining a deeper understanding of the research topics covered in BCAS publications. Positioned as a strategic S&T think tank, it is essential to explore its strategic focuses and how these have evolved over the past 36 years.\n\nTo answer these questions, we will construct a topic model using the dataset. For this purpose, we need to ensure we have the appropriate text data. The dataset includes article titles, keywords, and abstracts, but it also contains significant gaps: some articles have only titles, while others lack keywords or abstracts entirely.\n\nMoreover, since our primary interest lies in the technologies discussed, we must filter out publications that may introduce noise and complicate the interpretation of the topics. We will exclude the following irrelevant publications:\n\n- Covers (封面)\n- Tables of contents (目录)\n- Policy digests (政策速览)\n- Advertising (《中国科学院院刊》新媒体)\n- Prefaces (序言)\n- Technology progress digests lacking abstracts (科研进展)\n- Calls for papers\n- Blank entries\n\nThese publications do not contribute to a deeper understanding of the underlying topics and only serve to create noise.\n\nPreviously we have scraped data on similar articles. This data provides us with a form of clustering that we can leverage in our analysis. To summarize, the texts we will utilize for topic modeling will consist of a combination of **titles, keywords, abstracts, and references to similar articles**.\n\n***\n\nThere are various kinds of topics modeling techniques available. One of the older NLP techniques is Latent Dirichlet Allocation[^1], which was introduced in 2003 but is still widely used with multiple implementations in Python and R. We will use `BERTopic` library[^2], which\n\n> ... uses transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. \n\n\n[^1]: You can find an example of LDA implementation for Chinese using `gensim` in the code notebook.\n[^2]: See documentation [here](https://maartengr.github.io/BERTopic/index.html)\n\n# Building BERTopic Model\n\n## Data Preprocessing\n\nTo prepare text for modeling with BERTopic, we first need to tokenize the text. While standard tokenization methods involve splitting words by whitespace, this approach is unsuitable for Chinese. Instead, we’ll use `jieba` to effectively tokenize the text, allowing us to create a `CountVectorizer` alternative tailored for Chinese.\n\nAlthough the BERTopic documentation suggests that stopword removal is often unnecessary, I found that including stopwords significantly increased noise in the model. Therefore, we'll be incorporating a stopword list using the `cntext` library[^3] to improve the quality of the results.\n\nNext, we will write a function to tokenize text and remove stopwords.\n\n[^3]: For more details, see the Chinese documentation [here](https://github.com/hidadeng/cntext) and the English version [here](https://github.com/hiDaDeng/cntext/blob/main/english_readme.md).\n\nNext, we proceed to fitting the model. BERTopic captures semantic similarities by first transforming the input documents into numerical representations, known as embeddings. Typically, BERTopic leverages models from the sentence-transformers library; for our analysis, we employ the \"paraphrase-multilingual-MiniLM-L12-v2\" model, which is well-suited for capturing nuanced meanings across multiple languages.\n\nA critical decision point is determining the number of topics. While there are technical methods to estimate the optimal number of clusters, practical experience suggests that alignment with the research objective is paramount. We experimented with various models, exploring topic counts of 30, 40, 50, and 80. Fewer topics provide a broad overview, making it easier to identify overarching themes. However, this approach often leaves room for further topic refinement. Given that our goal is to discern specific technological trends, we require a higher level of detail.\n\nAfter several iterations, I determined that 100 topics strike the best balance between granularity and comprehensibility. In the code, we specify 101 topics, reserving one cluster for outliers to ensure that less relevant data does not skew the results. This approach allows us to capture a more nuanced and detailed picture of the technological landscape we are studying.\n\nOnce the model is fitted, we can begin examining the results. BERTopic generates a topic label by identifying the most relevant words within each topic, providing a concise representation of the underlying theme. This label serves as a quick summary, capturing the essence of the topic based on the most significant terms.\n\n## Analyzing Topics\n\nOnce we've identified the topics, we can begin a deeper analysis to uncover the distribution and significance of each. The most represented topics provide key insights into the dominant themes within the dataset. Among these, the most frequent topics include:\n\n1. **Innovation, Knowledge, Cooperation, Technology** (0_创新_知识_合作_科技)\n2. **Year, Introduction, Laboratory, Chinese Academy of Sciences** (1_年度_简介_实验室_中国科学院)\n3. **Innovation, Technology, Major Power, Reform** (2_创新_科技_强国_改革)\n4. **Biology, Organism, Cell, Gene** (3_生物学_生物_细胞_基因)\n5. **Wetland, Water Resources, Yangtze, Ecology** (4_湿地_水资源_长江_生态)\n\nThese topics highlight the emphasis on innovation, scientific research, and environmental concerns, reflecting the strategic focus areas of the publications.\n\nHowever, a significant portion of the documents—nearly 37%—fall into the outliers. This indicates a substantial number of articles that do not neatly fit into the defined topics, which could suggest a wide range of unique or less common subjects being covered. While this diversity is valuable, the high percentage of outliers also suggests that there may be a considerable amount of content that is either very specific or not directly aligned with the main thematic clusters.\n\nTo gain a deeper understanding of the topics generated by BERTopic, we can examine the topic word scores, which are based on cTF-IDF (class-based Term Frequency-Inverse Document Frequency) values. These scores highlight the words that are most representative and carry the most weight within each topic, giving us insights into the core themes of each cluster. By analyzing these topic word scores, we can form hypotheses about the central themes and the specific aspects of technology and innovation that each topic addresses.\n\nFor example, in Topic 0, the words with the highest cTF-IDF values are \"innovation,\" \"knowledge,\" \"cooperation,\" \"S&T\" (Science and Technology), and \"talent.\" These keywords suggest that the documents in this cluster likely focus on issues related to scientific and technological innovation. The presence of words like \"cooperation\" and \"talent\" indicates that the discussion may also involve the human and collaborative aspects of fostering innovation. This could include topics such as the collaboration between scientists within China and internationally, the development of talent necessary for advancing innovation, and the exchange of knowledge that drives scientific progress. The focus on collaboration within the topic related to \"innovation,\" \"knowledge,\" \"cooperation,\" \"S&T,\" and \"talent\" appears to be in contrast with the recent political direction in China, which emphasizes a more isolationist stance. This divergence raises interesting questions about the alignment between the strategic goals in the scientific community and the broader political climate.\n\nThe representation of Topic 1 with terms such as 'annual,' 'introduction,' 'laboratory,' 'Chinese Academy of Sciences,' and 'first prize' indicates that this cluster primarily focuses on the internal workings and institutional highlights of CAS. The presence of words like 'Chinese Academy of Sciences' and 'laboratory' suggests that the documents in this topic are centered around the internal operations of CAS. This could include details about new developments, research facilities, and organizational changes. 'Annual' points to documents that likely cover yearly reports or summaries. Terms like 'introduction' and 'first prize' imply that the documents might also discuss new leadership introductions, significant awards, or recognitions received by the academy. This cluster may reflect a focus on internal communications and documentation that serve to update stakeholders about the academy's progress, innovations, and administrative changes.\n\nBERTopic provides a functionality for visualizing the relationships between topics using an intertopic distance map. This map displays how closely related various topics are to one another, often revealing clusters of topics that share similar themes or concepts. In our case, the intertopic distance map suggests that many topics are closely aligned and could potentially be grouped into larger clusters.\n\nHowever, for the purposes of our project, which aims to uncover the specific technological focus of the publications, a more fine-grained approach is essential. By maintaining a higher number of distinct topics, we can better identify subtle differences and emerging trends within the technological landscape. This granularity allows us to capture the nuances in the research and development areas, providing a clearer picture of the strategic directions and innovation priorities reflected in the publications.\n\nWe can also observe the connections between topics using hierarchical clustering visualization.\n\nWhen we look at the topics, we see that some of them have overlapping keywords. The similarity matrix shows that a few topics are pretty similar. For example, the topics labeled 'innovation, knowledge, cooperation, S&T' (0_创新_知识_合作_科技) and 'innovation, S&T, major power, reform' (2_创新_科技_强国_改革) have high similarity scores. But there’s an important difference between them:\n\n1. **Topic 0 (创新_知识_合作_科技)**: This one is all about how innovation ties in with knowledge and collaboration. It’s focused on how scientific and tech progress connects with spreading knowledge and working together with other researchers.\n\n2. **Topic 2 (创新_科技_强国_改革)**: On the other hand, this topic is about the innovation process in the context of making China a major player in science and tech, along with the necessary reforms. It’s more about the political and strategic side of innovation, aimed at boosting the country’s overall strength and driving big changes.\n\nSo, even though both topics deal with innovation, Topic 0 is more about the practical aspects of knowledge and teamwork, while Topic 2 is more about the strategic and political side of making China a global leader in science and technology.\n\nThis shows that even if topics share some keywords and have high cosine similarity score, they can have very different focuses and themes.\n\nThis is exactly why I decided not to merge the topics. I want to avoid adding any more bias to the analysis. Keeping the topics separate helps us get a clearer, more nuanced view of the different themes and perspectives in the publications.\n\nBut we still need to deal with the outliers.\n\n## Reducing Outliers\n\nArticles often cover multiple themes, especially when they’re interdisciplinary. To handle this, we calculate the probability of each topic for every document. For this we need to set a probability threshold. This threshold helps us decide which topics are relevant enough to be included for each document, filtering out those with lower probabilities that might not add much value.[^4]\n\nBy using this method, we’ve managed to cut down the number of outliers from 36% to just 6%. We also end up with multiple topics assigned to each document, reflecting the real-world complexity and interdisciplinarity of strategic articles. This approach provides a more nuanced view of the content, aligning closely with how strategic topics often span across various fields. Using this method, we can avoid the hassle of merging topics manually since similar topics will naturally be assigned to the same documents.\n\n[^4]: We took some practical ideas on this matter from a great article [Topics per Class Using BERTopic](https://towardsdatascience.com/topics-per-class-using-bertopic-252314f2640) by Mariya Mansurova.  \n\n# Topic Interpretation\n\nThe original BERTopic labels give us a starting point, but to truly grasp their meaning, we need to translate these labels into more understandable terms.\n\nFor this, we can use advanced Large Language Models (LLMs), which excel at text processing. These models are great at predicting the next most likely word, which can help us craft clear and concise summaries of the topics.\n\nGoogle offers robust functionality with its Gemini-family models through AI Studio, and it's quite accessible (free, actually). All we need to do is generate an API key and connect to Gemini using the `google.generativeai` library. This will allow us to enhance our topic interpretations with a bit of AI assistance.\n\n## Prompt Engineering\n\nPrompt engineering is essential when working with Large Language Models (LLMs). To get the most accurate and useful results, we can craft our prompts carefully. \n\nBased on the advice from BERTopic developer Maarten Grootendorst, we can start with a prompt like this:\n\n> I have a topic that is described by the following keywords: [KEYWORDS]\n> In this topic, the following documents are a small but representative subset of all documents in the topic: [DOCUMENTS]\n> Based on the information above, please provide a description of this topic in the following format: topic: <description>\n\nThis method summarizes the most representative documents to generate a topic description. We’ll tweak this slightly to make the most of Gemini’s large context window by including all relevant documents (article titles) instead of just a subset. Additionally, we need to ensure that the descriptions are generated in both Chinese and English.\n\n## Summarizing Topics\n\nOnce we’ve crafted the prompt, the next step is to iterate through each topic and generate its summary.\n\nAlthough LLMs offer significant assistance, we still need to evaluate the topics ourselves.[^5] Some of the labels generated by Gemini may include excessive details or might be too similar to one another. Here is the updated list of topic labels:\n\n[^5]: The best option would be to consult experts from relevant science fields.\n\n# Classification by Science-Metrix Ontology\n\nNow that we have BCAS publications clustered into 100 distinct topics, we might find it helpful to step back and get a broader view. To do this, we can introduce an additional layer of metadata by classifying these topics into scientific fields.\n\nFor this purpose, we’ll leverage the ontology developed by [Science-Metrix](https://www.science-metrix.com/classification/). This classification system sorts scientific journals and articles into 5 domains, 20 fields, and 174 mutually exclusive subfields. Created in 2010 for the European Commission, this system was designed to address the absence of a universal classification scheme within the bibliometric community. It builds on existing journal classifications and was refined through a blend of algorithmic methods and expert judgment. In 2019, the classification was further enhanced:\n\n>  ... we further developed the journal-based classification into the article-level and hybrid versions. For the article-level classification, a scientific publication is attributed to a domain, field and subfield based on its title, abstract, keywords, author affiliation and the classification of its citations, using a deep neural network (an artificial intelligence technique). In the hybrid version, most articles are still classified at the journal level, except for those published in multidisciplinary journals (e.g., Science, Nature, PNAS and PLOS One), which are classified at the article level.\n\nApplying this classification system allows us to map our 100 topics to a structured framework, providing a clearer overview and uncovering broader trends. This approach offers flexibility, letting us choose different levels of granularity—whether analyzing data at the topic, subfield, field, or domain level.\n\n[^6]: Excel file with classification can be dowloaded [here](https://www.science-metrix.com/classification/).\n\nWe can use Gemini for the classification task as well! By using the English topics, we can prompt Gemini to identify the most appropriate subfield for each topic. The complete script, along with the prompt, looks like this:\n\nThe Gemini classification is pretty decent, but it’s not perfect—manual review and tweaking are still necessary. Here's an example of how we match topics to their most fitting subfields:\n\nNext we just need to match subfields with associated fields and domains. \n\n## Summary\n\nIn this chapter, we developed a topic model using the `BERTopic` library, opting not to manually merge topics. Instead, we utilized the Science-Metrix ontology, with assistance from Generative AI, for classification. This strategy allows us to maintain flexibility in our analysis, offering multi-level insights depending on the desired level of granularity. By assigning multiple topics to the BCAS articles, our approach more accurately reflects real-world scenarios, particularly in the context of S&T strategic documents where interdisciplinarity is often the norm.\n\nOur next step is to dive into the data using this new semantic layer.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"number-sections":true,"reference-location":"margin","output-file":"bcas_topic_model.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","fig-cap-location":"top","theme":{"light":"cosmo","dark":["cosmo","../theme-dark.scss"]},"smooth-scroll":true,"grid":{"body-width":"1000px","sidebar-width":"300px","margin-width":"300px"},"title":"Building Topic Model","jupyter":"python3","citation-location":"margin","toc-title":"Contents","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}