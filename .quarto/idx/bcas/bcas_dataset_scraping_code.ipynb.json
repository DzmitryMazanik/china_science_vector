{"title":"Analysing Website Structure","markdown":{"yaml":{"title":"Analysing Website Structure"},"headingText":"Scraping Data","containsRefs":false,"markdown":"\n\n\n\n\n\n\nFirst we should investigate the structure of the website. Article data we are looking for is in the section 'Past Issues' (过刊目录). \n\nEach issue page contains links to the full article  in PDF or HTML, and abstracts (摘要) which we will explore next.\nArticle pages contain a lot of data about publications:\n- title\n- date\n- issue\n- authors\n- affiliations\n- abstracts\n- keywords\n- associated fund projects\n- views and downloads statistics.\n\nRecent text data is presented in both Chinese and English, but older articles include only Chinese.\n\n\nThe old version of the BCAS website doesn't use JavaScript animations, so the classic `BeautifulSoup` library is enough at this stage.\n\n## Issues\n\nOur final goal is to retrieve the data on the articles. The website structure shows that the links to the articles can be found inside issues. So, our first move is to get the links to the issues.\n\n## Articles\n\nOnce we get the links to the issues, we can itearate through them and retrieve the links to the desired articles.\n\n## Article Data\n\nNow we can scrape the data for each BCAS article.\nTo do so we need to analyze the HTML structure of the pages and determine CSS selectors for the desired elements. \n\n### Define Elements\n\n### Similar Articles\n\nOur goal is to clasterize the articles using available texts -- so the more useful texts we have, the better. On the pages there is a section called \"Similar Articles\" (相似文献). The titles of similar publications may be useful for topic modeling, for it increases the chances for articles with similar referencies to appear in the same cluster.\n\nAccessing this data requires clicking a \"Similar Articles\" button, otherwise the text is not present on the page. We can simulate clicking (and a do lot of other useful stuff) using Selenium library. \n\n### Merge\n\n# Data Cleaning\n\nThe data we got is quite messy with lots of missing values, extra comas and other punctuation signs, some columns can be split into two to make more sense.\n\nIn this cases we can do some cleaning with Pandas and regular expressions.\n\n## Date & Issue\n\nFirst, we should transform strings like this '2024,39(1):0-0' into more meaningful form.\n\nThis string contains three features -- year, issue, and pages. The pattern for separating these features is consistent throughout the dataset, so by a simple split by coma and colon  we can create columns 'date', 'issue', and 'pages'.\n\n## Abstracts & Keywords\n\nNext we can remove irrelevant words like '中文摘要:', 'Abstract:', '中文关键词:', 'keywords:', '基金项目:'. This can be done through regular expression replacement. \n\n## Authors \n\nWe need to clean the author column as well and remove extra columns, numbers and other symbols like \"*\".\n\n## Organizations\n\nCleaning organization data is a tricky part, beacuse this part is the most inconsistent and messy.\nExample of the organization description: (1.北京大学 北京 100871;2.北京科学智能研究院 北京 100084). One articles can be written by several people from different instituitions. The affiliation info also includes data about city, postal codes and job titles in a lot of cases.\n\nWe don't need the postal codes and job titles for our research, but the city data can be useful for geospatial analysis.\n\nWe can get the relevant cities by checking what city from the list of Chinese cities (get it from Baidu) appears in the string.\n\nNext we need to remove the job titles from the strings.\n\nFor this purpose we can create a list of job titles which appear in the dataset.\n\nThe next important issue is the degree of detail about organizations we are interested in. The affiliation data includes in some cases the title of the head organization and a subdivision (lab, office, group, ). For now we will focus on the head organizations, e.g. if there is a string like '中国科学院自动化研究所 复杂系统管理与控制国家重点实验室' we count it like '中国科学院自动化研究所'.\n\nThis also can be achieved through regular exprssions.\n\nThe next step is to translate the data to English. Note that we need official English titles, so doing everything through machine translation is not the best fit.\n\nWe can try to retrieve some English titles from Baidu using Beautiful Soup again.\n\nNot all organizations are present on Baidu or have English translation. We can translate these with Google Translate through `deeptranslator`.\n\nFinally, we add some final text processing touches in Excel/Google Sheets and update the dataframe.\n\n## Fund Projects\n\nFinally, we need to clean the data about associated fund projects. \n","srcMarkdownNoYaml":"\n\n\n\n\n\n\nFirst we should investigate the structure of the website. Article data we are looking for is in the section 'Past Issues' (过刊目录). \n\nEach issue page contains links to the full article  in PDF or HTML, and abstracts (摘要) which we will explore next.\nArticle pages contain a lot of data about publications:\n- title\n- date\n- issue\n- authors\n- affiliations\n- abstracts\n- keywords\n- associated fund projects\n- views and downloads statistics.\n\nRecent text data is presented in both Chinese and English, but older articles include only Chinese.\n\n# Scraping Data\n\nThe old version of the BCAS website doesn't use JavaScript animations, so the classic `BeautifulSoup` library is enough at this stage.\n\n## Issues\n\nOur final goal is to retrieve the data on the articles. The website structure shows that the links to the articles can be found inside issues. So, our first move is to get the links to the issues.\n\n## Articles\n\nOnce we get the links to the issues, we can itearate through them and retrieve the links to the desired articles.\n\n## Article Data\n\nNow we can scrape the data for each BCAS article.\nTo do so we need to analyze the HTML structure of the pages and determine CSS selectors for the desired elements. \n\n### Define Elements\n\n### Similar Articles\n\nOur goal is to clasterize the articles using available texts -- so the more useful texts we have, the better. On the pages there is a section called \"Similar Articles\" (相似文献). The titles of similar publications may be useful for topic modeling, for it increases the chances for articles with similar referencies to appear in the same cluster.\n\nAccessing this data requires clicking a \"Similar Articles\" button, otherwise the text is not present on the page. We can simulate clicking (and a do lot of other useful stuff) using Selenium library. \n\n### Merge\n\n# Data Cleaning\n\nThe data we got is quite messy with lots of missing values, extra comas and other punctuation signs, some columns can be split into two to make more sense.\n\nIn this cases we can do some cleaning with Pandas and regular expressions.\n\n## Date & Issue\n\nFirst, we should transform strings like this '2024,39(1):0-0' into more meaningful form.\n\nThis string contains three features -- year, issue, and pages. The pattern for separating these features is consistent throughout the dataset, so by a simple split by coma and colon  we can create columns 'date', 'issue', and 'pages'.\n\n## Abstracts & Keywords\n\nNext we can remove irrelevant words like '中文摘要:', 'Abstract:', '中文关键词:', 'keywords:', '基金项目:'. This can be done through regular expression replacement. \n\n## Authors \n\nWe need to clean the author column as well and remove extra columns, numbers and other symbols like \"*\".\n\n## Organizations\n\nCleaning organization data is a tricky part, beacuse this part is the most inconsistent and messy.\nExample of the organization description: (1.北京大学 北京 100871;2.北京科学智能研究院 北京 100084). One articles can be written by several people from different instituitions. The affiliation info also includes data about city, postal codes and job titles in a lot of cases.\n\nWe don't need the postal codes and job titles for our research, but the city data can be useful for geospatial analysis.\n\nWe can get the relevant cities by checking what city from the list of Chinese cities (get it from Baidu) appears in the string.\n\nNext we need to remove the job titles from the strings.\n\nFor this purpose we can create a list of job titles which appear in the dataset.\n\nThe next important issue is the degree of detail about organizations we are interested in. The affiliation data includes in some cases the title of the head organization and a subdivision (lab, office, group, ). For now we will focus on the head organizations, e.g. if there is a string like '中国科学院自动化研究所 复杂系统管理与控制国家重点实验室' we count it like '中国科学院自动化研究所'.\n\nThis also can be achieved through regular exprssions.\n\nThe next step is to translate the data to English. Note that we need official English titles, so doing everything through machine translation is not the best fit.\n\nWe can try to retrieve some English titles from Baidu using Beautiful Soup again.\n\nNot all organizations are present on Baidu or have English translation. We can translate these with Google Translate through `deeptranslator`.\n\nFinally, we add some final text processing touches in Excel/Google Sheets and update the dataframe.\n\n## Fund Projects\n\nFinally, we need to clean the data about associated fund projects. \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":false,"output-file":"bcas_dataset_scraping_code.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","fig-cap-location":"top","theme":{"light":"cosmo","dark":["cosmo","../theme-dark.scss"]},"smooth-scroll":true,"grid":{"body-width":"1000px","sidebar-width":"300px","margin-width":"300px"},"title":"Analysing Website Structure"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}