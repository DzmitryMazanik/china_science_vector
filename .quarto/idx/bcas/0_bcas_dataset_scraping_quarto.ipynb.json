{"title":"Building Dataset","markdown":{"yaml":{"title":"Building Dataset","jupyter":"python3","format":{"html":{"grid":{"body-width":"1000px","sidebar-width":"300px","margin-width":"300px"},"toc":true,"toc-title":"Contents","page-layout":"full","code-overflow":"wrap"}},"number-sections":true,"reference-location":"margin","citation-location":"margin"},"headingText":"Scraping Data","containsRefs":false,"markdown":"\n\n\nThe old version of the BCAS website doesn't use Java Script animations, so the classic  `BeautifulSoup` library is enough at this stage.\n\n## Issues\n\nFirst we should investigate the structure of the website. Article data we are looking for is in the section 'Past Issues' (过刊目录). Each issue page contains links to the full article  in PDF or HTML, and abstracts (摘要) which we will explore next.\n\nOur final goal is to retrieve the data on the articles. The website structure shows that the links to the articles can be found inside issues. So, our first move is to get the links to the issues.\n\nLet's build a function to extract all issues URLs on the catalogue page. \n\nOnce the links are retrieved we need to save them in a txt file.\n\nFinally, we can execute the two functions and get the links to the issues.\n\n## Articles\n\nOnce we get the links to the issues, we can itearate through them and retrieve the links to the desired articles.\n\n## Article Data\n\nNow we can scrape the data for each BCAS article.\nTo do so we need to analyze the HTML structure of the pages and determine CSS selectors for the desired elements.\nArticle pages contain a lot of data about publications:\n\n- Title\n- Date\n- Issue\n- Authors\n- Affiliations\n- Abstracts\n- Keywords\n- Associated fund projects\n- Views and downloads statistics\n\n### Define Elements\n\nWe can start with a function which gets the desired element using beautifulsoup functionality.\n\nNext we locate each element through CSS selectors and parse with \"html.parser\". The function returns a dictionary with the extracted data.\n\nThe parsing process may take some time and connection errors (from both sides) can disturb the process. That's why it is better to wrap parsing in try-except block. If any exception occurs during the process, the function prints an error message with the URL and the specific error, and then returns None.\n\nNext we need to werite the scraped data into a csv file. \n\nFirst, we open a csv file in the write mode and define the fieldnames, which are the same as the fields of the dictionary in the get_data() function. We use a pipe \"|\" as a delimeter.\n\nNext we open the articles.txt with the URLs and iterate through them retrieving the data, which is tehn stored in the csv.\n\n\nDue to some formatting issues the parsed data can contain new lines \"/n\" in some cases, breaking the lines. We can fix this issues with a simple fucntion which replaces all line breaks (\\n) in the content with an empty string and then adds a new line break before each occurrence of 'http://old2022.bulletin.cas.cn/'. \n\n### Similar Articles\n\nOur goal is to clasterize the articles using available texts -- so the more useful texts we have, the better. On the pages there is a section called \"Similar Articles\" (相似文献). The titles of similar publications may be useful for topic modeling, for it increases the chances for articles with similar referencies to appear in the same cluster.\n\nAccessing this data requires clicking a \"Similar Articles\" button, otherwise the text is not present on the page. We can simulate clicking (and a do lot of other useful stuff) using `selenium` library. \n\n### Merge Dataset\n\nOnce we have one dataset with article metadata and the other with similar articles we can combine them into one.\nOne way to do so fast and easy is through Pandas.\n\nWe create two dataframes and merge them through SQL-like function. \n\nWe can explore the dataset a bit. \n\nAnd this is what the dataset woth references looks like.\n\n\n\nThere are multiple ways to combine these two datasets together. Now a SQL-like merge() function will do just fine.\n\n# Data Cleaning\n\nThe data we got is quite messy with lots of missing values, extra comas and other punctuation signs, some columns can be split into two to make more sense.\n\nIn this cases we can do some cleaning with `pandas` and regular expressions.\n\n## Date & Issue\n\nFirst, we should transform strings like this '2024,39(1):0-0' into more meaningful form.\n\nThis string contains three features -- year, issue, and pages. The pattern for separating these features is consistent throughout the dataset, so by a simple split by coma and colon  we can create columns 'date', 'issue', and 'pages'.\n\n## Abstracts & Keywords\n\nNext we can remove irrelevant words like '中文摘要:', 'Abstract:', '中文关键词:', 'keywords:', '基金项目:'. This can be done through regular expression replacement. \n\n## Authors \n\nWe need to clean the author column as well and remove extra columns, numbers and other symbols like \"*\".\n\nSome articles do not have a specified name of the author, but each publication is written by a person, so instead of NaN we fill missing values with \"not_specified\" text.\n\n## Organizations\n\nCleaning organization data is a tricky part, beacuse this part is the most inconsistent and messy.\nExample of the organization description: (1.北京大学 北京 100871;2.北京科学智能研究院 北京 100084). One articles can be written by several people from different instituitions. The affiliation info also includes data about city, postal codes and job titles in a lot of cases.\n\nWe don't need the postal codes and job titles for our research, but the city data can be useful for geospatial analysis.\n\nWe can get the relevant cities by checking what city from the list of Chinese cities (get it from Baidu) appears in the string.\n\nNext we need to remove the job titles from the strings.\n\nFor this purpose we can create a list of job titles which appear in the dataset.\n\nNow the strings will look something like this: \n\nThe next important issue is the degree of detail about organizations we are interested in. The affiliation data includes in some cases the title of the head organization and a subdivision (lab, office, group, ). For now we will focus on the head organizations, e.g. if there is a string like '中国科学院自动化研究所 复杂系统管理与控制国家重点实验室' we count it like '中国科学院自动化研究所'.\n\nThis also can be achieved through regular exprssions.\n\nThe next step is to translate the data to English. Note that we need official English titles, so doing everything through machine translation is not the best fit.\n\nWe can try to retrieve some English titles from Baidu using Beautiful Soup again.\n\nUnfortunately, not all organizations are present on Baidu or have English translation. We can translate these with Google Translate through `deep_translator` library.\n\nThis way we can get the translation of all organization titles:\n\nWe add some final touches in Excel/Google Sheets and update the dataframe. This dataset is small, so it is faster to just manually remove irrelevant symbols than to craft automated solution.\n\nFinally, we map years, cities and article titles to the dataframe and format the titles (change case, add abbreviations). The final result will look like this:\n\n## Fund Projects\n\nFinally, we need to clean the data about associated fund projects. \n\nThe final dataframe looks this way:\n","srcMarkdownNoYaml":"\n\n# Scraping Data\n\nThe old version of the BCAS website doesn't use Java Script animations, so the classic  `BeautifulSoup` library is enough at this stage.\n\n## Issues\n\nFirst we should investigate the structure of the website. Article data we are looking for is in the section 'Past Issues' (过刊目录). Each issue page contains links to the full article  in PDF or HTML, and abstracts (摘要) which we will explore next.\n\nOur final goal is to retrieve the data on the articles. The website structure shows that the links to the articles can be found inside issues. So, our first move is to get the links to the issues.\n\nLet's build a function to extract all issues URLs on the catalogue page. \n\nOnce the links are retrieved we need to save them in a txt file.\n\nFinally, we can execute the two functions and get the links to the issues.\n\n## Articles\n\nOnce we get the links to the issues, we can itearate through them and retrieve the links to the desired articles.\n\n## Article Data\n\nNow we can scrape the data for each BCAS article.\nTo do so we need to analyze the HTML structure of the pages and determine CSS selectors for the desired elements.\nArticle pages contain a lot of data about publications:\n\n- Title\n- Date\n- Issue\n- Authors\n- Affiliations\n- Abstracts\n- Keywords\n- Associated fund projects\n- Views and downloads statistics\n\n### Define Elements\n\nWe can start with a function which gets the desired element using beautifulsoup functionality.\n\nNext we locate each element through CSS selectors and parse with \"html.parser\". The function returns a dictionary with the extracted data.\n\nThe parsing process may take some time and connection errors (from both sides) can disturb the process. That's why it is better to wrap parsing in try-except block. If any exception occurs during the process, the function prints an error message with the URL and the specific error, and then returns None.\n\nNext we need to werite the scraped data into a csv file. \n\nFirst, we open a csv file in the write mode and define the fieldnames, which are the same as the fields of the dictionary in the get_data() function. We use a pipe \"|\" as a delimeter.\n\nNext we open the articles.txt with the URLs and iterate through them retrieving the data, which is tehn stored in the csv.\n\n\nDue to some formatting issues the parsed data can contain new lines \"/n\" in some cases, breaking the lines. We can fix this issues with a simple fucntion which replaces all line breaks (\\n) in the content with an empty string and then adds a new line break before each occurrence of 'http://old2022.bulletin.cas.cn/'. \n\n### Similar Articles\n\nOur goal is to clasterize the articles using available texts -- so the more useful texts we have, the better. On the pages there is a section called \"Similar Articles\" (相似文献). The titles of similar publications may be useful for topic modeling, for it increases the chances for articles with similar referencies to appear in the same cluster.\n\nAccessing this data requires clicking a \"Similar Articles\" button, otherwise the text is not present on the page. We can simulate clicking (and a do lot of other useful stuff) using `selenium` library. \n\n### Merge Dataset\n\nOnce we have one dataset with article metadata and the other with similar articles we can combine them into one.\nOne way to do so fast and easy is through Pandas.\n\nWe create two dataframes and merge them through SQL-like function. \n\nWe can explore the dataset a bit. \n\nAnd this is what the dataset woth references looks like.\n\n\n\nThere are multiple ways to combine these two datasets together. Now a SQL-like merge() function will do just fine.\n\n# Data Cleaning\n\nThe data we got is quite messy with lots of missing values, extra comas and other punctuation signs, some columns can be split into two to make more sense.\n\nIn this cases we can do some cleaning with `pandas` and regular expressions.\n\n## Date & Issue\n\nFirst, we should transform strings like this '2024,39(1):0-0' into more meaningful form.\n\nThis string contains three features -- year, issue, and pages. The pattern for separating these features is consistent throughout the dataset, so by a simple split by coma and colon  we can create columns 'date', 'issue', and 'pages'.\n\n## Abstracts & Keywords\n\nNext we can remove irrelevant words like '中文摘要:', 'Abstract:', '中文关键词:', 'keywords:', '基金项目:'. This can be done through regular expression replacement. \n\n## Authors \n\nWe need to clean the author column as well and remove extra columns, numbers and other symbols like \"*\".\n\nSome articles do not have a specified name of the author, but each publication is written by a person, so instead of NaN we fill missing values with \"not_specified\" text.\n\n## Organizations\n\nCleaning organization data is a tricky part, beacuse this part is the most inconsistent and messy.\nExample of the organization description: (1.北京大学 北京 100871;2.北京科学智能研究院 北京 100084). One articles can be written by several people from different instituitions. The affiliation info also includes data about city, postal codes and job titles in a lot of cases.\n\nWe don't need the postal codes and job titles for our research, but the city data can be useful for geospatial analysis.\n\nWe can get the relevant cities by checking what city from the list of Chinese cities (get it from Baidu) appears in the string.\n\nNext we need to remove the job titles from the strings.\n\nFor this purpose we can create a list of job titles which appear in the dataset.\n\nNow the strings will look something like this: \n\nThe next important issue is the degree of detail about organizations we are interested in. The affiliation data includes in some cases the title of the head organization and a subdivision (lab, office, group, ). For now we will focus on the head organizations, e.g. if there is a string like '中国科学院自动化研究所 复杂系统管理与控制国家重点实验室' we count it like '中国科学院自动化研究所'.\n\nThis also can be achieved through regular exprssions.\n\nThe next step is to translate the data to English. Note that we need official English titles, so doing everything through machine translation is not the best fit.\n\nWe can try to retrieve some English titles from Baidu using Beautiful Soup again.\n\nUnfortunately, not all organizations are present on Baidu or have English translation. We can translate these with Google Translate through `deep_translator` library.\n\nThis way we can get the translation of all organization titles:\n\nWe add some final touches in Excel/Google Sheets and update the dataframe. This dataset is small, so it is faster to just manually remove irrelevant symbols than to craft automated solution.\n\nFinally, we map years, cities and article titles to the dataframe and format the titles (change case, add abbreviations). The final result will look like this:\n\n## Fund Projects\n\nFinally, we need to clean the data about associated fund projects. \n\nThe final dataframe looks this way:\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"number-sections":true,"reference-location":"margin","output-file":"0_bcas_dataset_scraping_quarto.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","fig-cap-location":"top","theme":{"light":"cosmo","dark":["cosmo","../theme-dark.scss"]},"smooth-scroll":true,"grid":{"body-width":"1000px","sidebar-width":"300px","margin-width":"300px"},"title":"Building Dataset","jupyter":"python3","citation-location":"margin","toc-title":"Contents","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}